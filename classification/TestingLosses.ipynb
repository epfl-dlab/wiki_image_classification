{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops.losses import util as tf_losses_util\n",
    "from tensorflow.python.autograph.impl import api as autograph\n",
    "from tensorflow.python.autograph.core import ag_ctx\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctionWrapper(Loss):\n",
    "  \"\"\"Wraps a loss function in the `Loss` class.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               fn,\n",
    "               name=None,\n",
    "               **kwargs):\n",
    "    \n",
    "    super(LossFunctionWrapper, self).__init__(name=name)\n",
    "    self.fn = fn\n",
    "    self._fn_kwargs = kwargs\n",
    "\n",
    "  def call(self, y_true, y_pred):\n",
    "    \"\"\"Invokes the `LossFunctionWrapper` instance.\n",
    "    Args:\n",
    "      y_true: Ground truth values.\n",
    "      y_pred: The predicted values.\n",
    "    Returns:\n",
    "      Loss values per sample.\n",
    "    \"\"\"\n",
    "    if tensor_util.is_tensor(y_pred) and tensor_util.is_tensor(y_true):\n",
    "      y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n",
    "          y_pred, y_true)\n",
    "    ag_fn = autograph.tf_convert(self.fn, ag_ctx.control_status_ctx())\n",
    "    return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {}\n",
    "    for k, v in six.iteritems(self._fn_kwargs):\n",
    "      config[k] = K.eval(v) if tf_utils.is_tensor_or_variable(v) else v\n",
    "    base_config = super(LossFunctionWrapper, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class BinaryFocalCrossentropy(LossFunctionWrapper):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, from_logits=False, axis=-1):\n",
    "        super().__init__(fn=binary_focal_crossentropy)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.axis = axis\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "def binary_focal_crossentropy(y_true, y_pred, alpha=0.25, gamma=2, from_logits=True, axis=-1):\n",
    "    y_pred = ops.convert_to_tensor_v2(y_pred)\n",
    "    if from_logits:\n",
    "        # Transform logits to probabilities\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        y_pred = sigmoid(y_pred)\n",
    "    else:\n",
    "        # Clip probabilities for numerical stability\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "    y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    term_1 = y_true * alpha * tf.math.pow(1 - y_pred, gamma) * tf.math.log(y_pred)\n",
    "    term_0 = (1 - y_true) * (1 - alpha) * tf.math.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "    focal_ce = -(term_1 + term_0)\n",
    "\n",
    "    return K.mean(focal_ce, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51013"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y_true = np.array([0, 1, 0, 0])\n",
    "y_pred = np.array([-18.6, 0.51, 2.94, -12.8])\n",
    "bce = BinaryFocalCrossentropy()\n",
    "round(bce(y_true, y_pred).numpy(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data, y_pred and y_true have the format: batch_size X nr_classes (rows X columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00337, time: 0.00016617774963378906\n",
      "0.00337, time: 0.0002703666687011719\n",
      "0.00337, time: 0.0003037452697753906\n"
     ]
    }
   ],
   "source": [
    "# Test the implementation of binary_focal_crossentropy() doing it by hand below\n",
    "y_true = np.array([1., 0., 1., 1., 1., 0.])\n",
    "y_pred = np.array([0.8, 0.2, 0.7, 0.9, 0.8, 0.1])\n",
    "start = time.time()\n",
    "bce = BinaryFocalCrossentropy()\n",
    "end = time.time()\n",
    "print(f'{round(bce(y_true, y_pred).numpy(), 5)}, time: {end-start}')\n",
    "\n",
    "\n",
    "# By 'hand':\n",
    "y_true = np.array([1., 0., 1., 1., 1., 0.])\n",
    "y_pred = np.array([0.8, 0.2, 0.7, 0.9, 0.8, 0.1])\n",
    "alpha = 0.25\n",
    "gamma = 2\n",
    "term_1 = 0\n",
    "term_0 = 0\n",
    "start = time.time()\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i] == 1:\n",
    "        term_1 += alpha * (1 - y_pred[i])**gamma * np.log(y_pred[i])\n",
    "    else:\n",
    "        term_0 += (1 - alpha) * y_pred[i]**gamma * np.log(1 - y_pred[i])\n",
    "focal_loss = -(term_1 + term_0) / len(y_pred)\n",
    "end = time.time()\n",
    "print(f'{round(focal_loss, 5)}, time: {end-start}')\n",
    "\n",
    "# Or vectorized:\n",
    "start = time.time()\n",
    "term_1 = y_true * alpha * (1 - y_pred)**gamma * np.log(y_pred)\n",
    "term_0 = (1 - y_true) * (1 - alpha) * y_pred**gamma * np.log(1 - y_pred)\n",
    "focal_loss = np.mean(-(term_1 + term_0))\n",
    "end = time.time()\n",
    "print(f'{round(focal_loss, 5)}, time: {end-start}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.22314342631421757>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ops.convert_to_tensor_v2(y_pred)\n",
    "y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "\n",
    "term_0 = (1 - y_true) * K.log(1 - y_pred + K.epsilon())\n",
    "term_1 = y_true * K.log(y_pred + K.epsilon())\n",
    "bce = -K.mean(term_0 + term_1)\n",
    "bce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BFCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-0.7747613922315705>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ops.convert_to_tensor_v2(y_pred)\n",
    "y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "alpha = 0.25\n",
    "gamma = 2\n",
    "\n",
    "term_1 = -alpha * K.pow(1 - y_pred, gamma) * K.log(y_pred + K.epsilon())\n",
    "\n",
    "term_0 = -(1 - alpha) * K.pow(y_pred, gamma) * K.log(1 - y_pred + K.epsilon())\n",
    "\n",
    "bfce = -K.mean(term_0 + term_1)\n",
    "bfce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a scenario where we have 1M samples with label 1 and with p_t 0.99, and 10 samples with label 0 predicted with p_t 0.01. Then we have the following two cases happening in cross entropy and focal loss cross entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of the minority class in the CE loss is: -0.4561%\n",
      "The fraction of the minority class in the CE loss is: -93.7382%\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy:\n",
    "ce_label_1 = 10 * np.log(0.01) # 1_000_000 images\n",
    "ce_label_0 = 1_000_000 * np.log(0.99) # 10 images\n",
    "total_ce = -(ce_label_1 + ce_label_0)\n",
    "print(f'The fraction of the minority class in the CE loss is: {ce_label_1/total_ce*100:.4f}%')\n",
    "\n",
    "# Focal cross entropy:\n",
    "alpha = 0.25\n",
    "gamma = 2\n",
    "fce_label_1 = 10 * (-alpha) * ((1 - 0.01)**gamma) * np.log(0.01) # 1_000_000 images\n",
    "fce_label_0 = 1_000_000 * (-(1 - alpha)) * (0.01**gamma) * np.log(1- 0.01) # 10 images\n",
    "total_fce = -(fce_label_1 + fce_label_0)\n",
    "print(f'The fraction of the minority class in the CE loss is: {fce_label_1/total_fce*100:.4f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice thus that the minority class, in the Focal Cross Estropy Loss function, receives much less importance. Thus, the gradients will be updated to repair the loss on the less known classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7f323a1c2c8b3d38dc94a01188981c510c9b5df10e2cc2d7fa4f2b45d318cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
