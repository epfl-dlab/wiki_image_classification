{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import logging, sys\n",
    "\n",
    "from headParsing import find_head\n",
    "from iteration_utilities import duplicates, unique_everseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\n",
    "logging.basicConfig(filename='categories.log',\n",
    "                            filemode='a',\n",
    "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                            datefmt='%H:%M:%S',\n",
    "                            level=logging.DEBUG)\n",
    "\n",
    "logging.info(\"Label querying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "CATEGORIES_PATH = '/scratch/WikipediaImagesTaxonomy/commonswiki-20220220-category-network.parquet'\n",
    "FILES_PATH = '/scratch/WikipediaImagesTaxonomy/commonswiki-20220220-files.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taxonomy:\n",
    "    def __init__(self, G=None):\n",
    "        if(G):\n",
    "            self.G = G\n",
    "\n",
    "    def load_categories(self, path):\n",
    "        '''\n",
    "        Load categories from path and build the category graph.\n",
    "        '''\n",
    "        self.build_category_graph(pd.read_parquet(path))\n",
    "    \n",
    "    def build_category_graph(self, categories):\n",
    "        '''\n",
    "        Build the category graph, starting from the DataFrame extracted by processing dumps\n",
    "        '''\n",
    "        categories = categories.set_index('title')\n",
    "        # Build DiGraph from adjacency matrix\n",
    "        G = nx.DiGraph(categories.parents.to_dict())\n",
    "        nx.set_node_attributes(G, dict(zip(categories.index, categories[['id', 'hiddencat']].to_dict(orient='records'))))\n",
    "        depth = {node: len(sps) for node, sps in nx.shortest_path(G, target='CommonsRoot').items()}\n",
    "        nx.set_node_attributes(G, depth, name='depth')\n",
    "        self.G = G\n",
    "\n",
    "    def reset_labels(self):\n",
    "        '''\n",
    "        Reset labels and discovery status for each node.\n",
    "        '''\n",
    "        nx.set_node_attributes(self.G, {node: {'visited': False, 'labels': set()} for node in self.G.nodes})\n",
    "        self.visited_nodes = 0\n",
    "\n",
    "    def set_taxonomy(self, taxonomy):\n",
    "        '''\n",
    "        Set an ORES-like taxonomy, mapping labels to high-level categories.\n",
    "        '''\n",
    "        self.taxonomy = taxonomy\n",
    "        self.reset_labels()\n",
    "        for label, categories in taxonomy.items():\n",
    "            for category in categories:\n",
    "                self.visited_nodes += 1\n",
    "                self.G.nodes[category]['visited'] = True\n",
    "                self.G.nodes[category]['labels'].add(label)\n",
    "    \n",
    "    def get_head(self, category):\n",
    "        '''\n",
    "        Get or compute the lexical head of a given category.\n",
    "        '''\n",
    "        if('head' in self.G.nodes[category]):\n",
    "            head = self.G.nodes[category]['head']\n",
    "        else:\n",
    "            head = find_head(category)\n",
    "            self.G.nodes[category]['head'] = head\n",
    "        return head\n",
    "\n",
    "\n",
    "    def get_label(self, category, how='heuristics'):\n",
    "        '''\n",
    "        Get the label corresponding to a specific category, passed as string.\n",
    "\n",
    "        Params:\n",
    "            how (string): decision scheme to recursively query parents. \n",
    "                all: all parents are queried\n",
    "                naive: only the first parent is queried\n",
    "                heuristics: decision based on the set of heuristics described in ??\n",
    "        '''\n",
    "        assert isinstance(category, str)\n",
    "\n",
    "        if(self.G.nodes[category]['visited']):\n",
    "            logging.debug('Found ' + category + ' with label ' + str(self.G.nodes[category]['labels']))\n",
    "            return self.G.nodes[category]['labels']\n",
    "        \n",
    "        else:\n",
    "            self.G.nodes[category]['visited'] = True\n",
    "            self.visited_nodes += 1\n",
    "            logging.debug(str(self.visited_nodes) + ' - Searching for ' + category + '...')\n",
    "\n",
    "            if(how == 'all'):\n",
    "                for parent in self.G.neighbors(category):\n",
    "                    self.G.nodes[category]['labels'].update(self.get_label(parent))\n",
    "                return self.G.nodes[category]['labels']\n",
    "            elif(how=='naive'):\n",
    "                pass\n",
    "            elif(how=='heuristics'):\n",
    "                # 1. Hidden category\n",
    "                if(self.G.nodes[category]['hiddencat']):\n",
    "                    return set()\n",
    "\n",
    "                # 2. Lexical head\n",
    "                heads = [self.get_head(category)]\n",
    "                for parent in self.G.neighbors(category):\n",
    "                    heads.append(self.get_head(parent))\n",
    "\n",
    "                # Try to match over complete lexical heads or subsets\n",
    "                while(1):\n",
    "                    common_heads = list(unique_everseen(duplicates(heads)))\n",
    "\n",
    "                    # Break if found a common head or all the heads are already 1 word long\n",
    "                    if(common_heads or (cmax:=max(map(lambda x: len(x.split())), heads)) == 1):\n",
    "                        break\n",
    "\n",
    "                    # Remove 1 word from the longest composite heads\n",
    "                    for i, head in enumerate(heads):\n",
    "                        head_words = head.split()\n",
    "                        if(len(head_words) == cmax):\n",
    "                            heads[i] = ' '.join(head_words[1:]).capitalize()\n",
    "\n",
    "                logging.debug('\\tFound lexical heads: ' + str(common_heads))\n",
    "                for common_head in common_heads:\n",
    "                    if(common_head in self.G):\n",
    "                        self.G.nodes[category]['labels'].update(self.get_label_heuristics(common_head))\n",
    "                    else:\n",
    "                        logging.debug('Lexical head ' + str(common_head) + ' not found')\n",
    "                \n",
    "                # Will be empty if no common_head is found, if the common_heads are\n",
    "                # all not valid category names, hidden categories or already visited \n",
    "                # (including the current category)\n",
    "                if(self.G.nodes[category]['labels']):\n",
    "                    return self.G.nodes[category]['labels']\n",
    "\n",
    "                # 3. is_a or subcategory_of (temp: depth check)\n",
    "                return self.G.nodes[category]['labels']\n",
    "            else:\n",
    "                raise ValueError('Invalid \"how\" option')\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = Taxonomy()\n",
    "taxonomy.load_categories(CATEGORIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = {'Nature': ['Animalia', 'Fossils', 'Landscapes', 'Marine organisms', 'Plantae', 'Weather'],\n",
    "                'Society/Culture': ['Art', 'Belief', 'Entertainment', 'Events', 'Flags', 'Food', 'History', 'Language', 'Literature', 'Music', 'Objects', 'People', 'Places', 'Politics', 'Sports'],\n",
    "                'Science': ['Astronomy', 'Biology', 'Chemistry', 'Earth sciences', 'Mathematics', 'Medicine', 'Physics', 'Technology'],\n",
    "                'Engineering': ['Architecture', 'Chemical engineering', 'Civil engineering', 'Electrical engineering', 'Environmental engineering', 'Geophysical engineering', 'Mechanical engineering', 'Process engineering']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy.set_taxonomy(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy.get_label('Stellar astronomy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy.get_label('Astronomy by city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'Comedy films of the United States'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04bf3c03e93e0e94a1d038aa36107c2a70cb015ab0da2208309d202be833925b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('francesco': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
