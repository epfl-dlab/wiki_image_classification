{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6877c6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c4818f-bded-4947-87c0-960bfe19575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 16:28:21 WARN Utils: Your hostname, iccluster111 resolves to a loopback address: 127.0.1.1; using 10.90.36.41 instead (on interface eno1)\n",
      "22/03/24 16:28:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/salvi/.conda/envs/francesco/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/salvi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/salvi/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0d95e5af-11b5-42e8-9100-ac3202700782;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.8.0 in central\n",
      "\tfound commons-io#commons-io;2.6 in central\n",
      "\tfound org.glassfish.jaxb#txw2;2.3.2 in central\n",
      ":: resolution report :: resolve 328ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.8.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.6 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;2.3.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0d95e5af-11b5-42e8-9100-ac3202700782\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "22/03/24 16:28:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import urllib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import wikitextprocessor as wtp\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "conf = pyspark.SparkConf().setMaster(\"local[10]\").setAll([\n",
    "                                   ('spark.jars.packages', 'com.databricks:spark-xml_2.12:0.8.0'),\n",
    "                                   ('spark.executor.memory', '8g'),\n",
    "                                   ('spark.driver.memory','10g'),\n",
    "                                   ('spark.driver.maxResultSize', '50G'),\n",
    "                                   ('spark.executor.heartbeatInterval', '60s'),\n",
    "                                   ('spark.network.timeout', '61s')\n",
    "                                  ])\n",
    "# create the session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb00d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.setLogLevel('DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3decbae-9d5d-44e5-8c4c-2f145f01b90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.36.41:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff173278a00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e703d84c-8ad2-41cc-b56b-878e156247b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "COMMONS_DUMP = '/scratch/WikipediaImagesTaxonomy/commonswiki-20220220-pages-articles-multistream.xml.bz2'\n",
    "# COMMONS_DUMP_REDUCED = '../../commonswiki-20220220-pages-articles-multistream1.xml-p1p1500000.bz2'\n",
    "COMMONS_DUMP_REDUCED = '../../commonswiki-20220220-pages-articles-multistream6.xml-p114543930p115400363.bz2'\n",
    "# list of chunks of the WIT dataset\n",
    "WIT_DATASET = ['../../wit_v1.train.all-1percent_sample.tsv.gz']\n",
    "\n",
    "# Outputs (ideally in scratch/, but I don't have permissions)\n",
    "TEMPLATES_DUMP = '../../commonswiki-20220220-templates-modules.xml'\n",
    "CATEGORIES_PATH = '../../commonswiki-20220220-category-network.parquet'\n",
    "FILES_PATH = '../../commonswiki-20220220-files.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb88d1b-3633-45c2-8b12-003f27ae5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/epfl-dlab/WikiPDA/blob/master/PaperAndCode/TopicsExtractionPipeline/GenerateDataframes.py\n",
    "def normalize_title(title, dumps=True):\n",
    "    \"\"\" Replace _ with space, remove anchor and namespace prefix, capitalize \"\"\"\n",
    "    title = urllib.parse.unquote(title)\n",
    "    if(dumps):\n",
    "        title = title.split(':', 1)[1]\n",
    "    title = title.strip()\n",
    "    if len(title) > 0:\n",
    "        title = title[0].upper() + title[1:]\n",
    "    n_title = title.replace(\"_\", \" \")\n",
    "    if '#' in n_title:\n",
    "        n_title = n_title.split('#')[0]\n",
    "    return n_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9e356-a201-4e56-af32-15d5908a6e40",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba4845f-bf35-4e09-9408-400b9e571eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "commons_categories_raw = spark.read.format('com.databricks.spark.xml') \\\n",
    "                                .options(rowTag='page').load(COMMONS_DUMP_REDUCED).filter(\"ns = '14'\")\n",
    "# commons_categories_raw.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b404d1e2-de8b-4aff-92e4-da006ea15a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- ns: long (nullable = true)\n",
      " |-- redirect: struct (nullable = true)\n",
      " |    |-- _VALUE: string (nullable = true)\n",
      " |    |-- _title: string (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- minor: string (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- parentid: long (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _bytes: long (nullable = true)\n",
      " |    |    |-- _xml:space: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commons_categories_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e7fac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Scholars of Albania': 'Scholars from Albania',\n",
       " 'IRT Substation 17': 'Dyckman-Hillside Substation (Substation 17)',\n",
       " 'The Northern Mariana Islands': 'Northern Mariana Islands'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a dictionary of redirects (old_title -> redirect_title)\n",
    "category_redirects = {normalize_title(r.title): normalize_title(r.redirect._title) \n",
    "                      for r in commons_categories_raw.filter('redirect is not null').collect()}\n",
    "category_redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef081ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_regex = re.compile('\\[\\[(Category:[^\\|]*?)(?:\\|.*?)*\\]\\]')\n",
    "hiddencat_regex = re.compile('__HIDDENCAT__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55b6d41d-d11d-4ff8-ad2c-ba7f2c431073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildsAccumulator(AccumulatorParam):\n",
    "    '''\n",
    "    Accumulator for childs: a dictionary mapping each category to its childs\n",
    "    '''\n",
    "    def zero(self, value):\n",
    "        return defaultdict(list)\n",
    "\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for key, value in val2.items():\n",
    "            val1[key] += value\n",
    "        return val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03bf2dad-11f8-4400-8304-7d767b297193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_category(row):\n",
    "    '''\n",
    "    Extract the details of a category\n",
    "    '''\n",
    "    title = normalize_title(row.title)\n",
    "    text = row.revision.text._VALUE\n",
    "        \n",
    "    parents = re.findall(categories_regex, text) if text else []\n",
    "    parents = [category_redirects[normalize_title(parent)] if normalize_title(parent) \n",
    "               in category_redirects.keys() else normalize_title(parent) for parent in parents]\n",
    "    global acc\n",
    "    if parents:\n",
    "        acc += {parent: [title] for parent in parents}\n",
    "    return Row(\n",
    "        id=row.id,\n",
    "        title=title,\n",
    "        parents=parents,\n",
    "        hiddencat=re.search(hiddencat_regex, text) is not None if text else False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "562fa86b-1ed3-4d36-84cc-76d3d16a79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of the processed categories DataFrame\n",
    "schema_cat = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                         StructField(\"title\", StringType(), True),\n",
    "                         StructField(\"parents\", ArrayType(StringType()), True),\n",
    "                         StructField(\"hiddencat\", BooleanType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd88f17e-9c3a-432a-bd73-8e10848623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ignore redirect categories, eventually remapping parents to their redirects\n",
    "acc = sc.accumulator(defaultdict(list), ChildsAccumulator())\n",
    "categories_clean = spark.createDataFrame(commons_categories_raw.filter('redirect is null')\\\n",
    "                                            .rdd.map(extract_category).filter(lambda r: r is not None), \n",
    "                                         schema=schema_cat)\n",
    "\n",
    "# commons_categories_raw.unpersist()\n",
    "# categories_clean.persist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9160c232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Test with write instead of collect - Workaround for the fact that the value of acc is used before it is filled\n",
    "\n",
    "TEMP_PATH = '../../dump.xml'\n",
    "\n",
    "categories_clean.write.format(\"com.databricks.spark.xml\").mode(\"overwrite\")\\\n",
    "                                         .options(rowTag='page', rootTag='pages').save(TEMP_PATH)\n",
    "\n",
    "# Remove files\n",
    "shutil.rmtree(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60848465-2d30-4ac9-b6e9-9f7138b5e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for the fact that the value of acc is used before it is filled\n",
    "# categories_clean.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca709dc-e7d5-4391-98ea-6dbc6c69888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_childs = StructType([StructField('title', StringType(), True),\n",
    "                            StructField('childs', ArrayType(StringType(), True), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b54405a-86cc-4617-a770-75987e4f0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "childs_df = spark.createDataFrame(acc.value.items(), schema=schema_childs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1291a1a4-a05f-4480-b23a-24971a5f82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = categories_clean.alias('c').join(childs_df, categories_clean.title==childs_df.title).select('c.*', 'childs')\n",
    "# categories_clean.unpersist();\n",
    "# categories.persist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ee01b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 16:39:14 WARN TaskSetManager: Stage 15 contains a task of very large size (1241 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "categories.write.mode(\"overwrite\").parquet(CATEGORIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dba6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = spark.read.parquet(CATEGORIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2104faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+---------+--------------------+\n",
      "|       id|               title|             parents|hiddencat|              childs|\n",
      "+---------+--------------------+--------------------+---------+--------------------+\n",
      "|115179492|1 manat of Turkme...|[Currencies with ...|    false|[1 manat banknote...|\n",
      "|115178624|10 manat of Turkm...|[Currencies with ...|    false|[10 manat banknot...|\n",
      "|115179595|100 manat of Turk...|[Currencies with ...|    false|[100 manat bankno...|\n",
      "|115075609|   1000 D of Germany|        [MaK 1000 D]|    false|[1000 D of Hafen-...|\n",
      "|114952756|10th-century scul...|[Sculptures in Li...|    false|[10th-century scu...|\n",
      "|114876524|10th-century work...|[Works in Baden-W...|    false|[10th-century pai...|\n",
      "|115032212|11th-century BC w...|[11th-century BC ...|    false|[11th-century BC ...|\n",
      "|114952761|11th-century scul...|[Sculptures in Li...|    false|[11th-century scu...|\n",
      "|114717666|  1200s art in Italy|                  []|    false|[1200s art in Genoa]|\n",
      "|115394315|12th-century Budd...|[12th-century Bud...|    false|[12th-century Bud...|\n",
      "|114727863|1300s reliefs in ...|[Reliefs in Italy...|    false|[1300s reliefs in...|\n",
      "|114958881|13th-century pain...|[13th-century pai...|    false|[13th-century fre...|\n",
      "|115084905|140s BC architect...|[2nd-century BC a...|    false|[140s BC architec...|\n",
      "|114727545|1420s reliefs in ...|[Reliefs in Italy...|    false|[1420s reliefs in...|\n",
      "|114712778|  1490s art in Genoa|[Genoa in the 149...|    false|[Bronze bust of G...|\n",
      "|114873546|14th-century fres...|[Frescos in Tusca...|    false|[14th-century fre...|\n",
      "|114873152|14th-century fres...|[Frescos in Venet...|    false|[14th-century fre...|\n",
      "|114742301|14th-century pain...|[The Holy Spirit ...|    false|[14th-century pai...|\n",
      "|115129798|       1551 in Genoa|                  []|    false|[Luca Spinola (14...|\n",
      "|114843022|15th-century work...|[15th-century wor...|    false|[15th-century pai...|\n",
      "+---------+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8aaa645e-3028-4e98-a1da-0010cdbb4aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 16:36:42 WARN TaskSetManager: Stage 4 contains a task of very large size (1241 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hidden_categories = categories.filter('hiddencat is True').select('title').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "105b43b1-da14-42e7-b0a3-71562e36f974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Valid SVG created with Text Editor',\n",
       " 'Valid SVG created with Text Editor',\n",
       " 'Valid SVG created with Text Editor',\n",
       " 'Valid SVG created with Adobe Illustrator',\n",
       " 'Washington Park Hist Dist Image by CLight',\n",
       " 'Giornali storici digitalizzati by Gianluca T.',\n",
       " 'Photographs by Gaudi Renanda',\n",
       " 'Files by User',\n",
       " 'Files by User',\n",
       " 'Schereville Image by Chris Light']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_categories[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cba5ca-1e47-4740-beb3-7cd854dd3cde",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae495bfe-99a2-4735-b105-cfc50d424494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "commons_files_raw = spark.read.format('com.databricks.spark.xml') \\\n",
    "                                .options(rowTag='page').load(COMMONS_DUMP_REDUCED)\\\n",
    "                                .filter(\"ns = '6'\")\n",
    "# commons_files_raw.persist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0437442b-9d70-4a6f-a20c-bd386a343332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Build a dictionary of redirects\n",
    "file_redirects = {normalize_title(r.title): normalize_title(r.redirect._title)\n",
    "                  for r in commons_files_raw.filter('redirect is not null').collect()}\n",
    "# file_redirects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a698ca9-9a92-40ec-8d44-0148c12050f5",
   "metadata": {},
   "source": [
    "For now, we consider only the images that appear in en.wikipedia, discarding all the others. We can also ignore redirects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a214f82d-6ea7-4fc5-9bb6-7d8746af67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_image_names = []\n",
    "\n",
    "for chunk in WIT_DATASET:\n",
    "    wiki_image_names += pd.read_csv(chunk, sep=\"\\t\").query(\"language == 'en'\")\\\n",
    "                            .image_url.apply(lambda r: normalize_title(r.split('/')[-1], False)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21abc0dd-f651-444a-95c5-6c9bb7cde966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only unique values\n",
    "wiki_image_names = set(wiki_image_names)\n",
    "\n",
    "# Remap redirects\n",
    "wiki_image_names = {file_redirects[name] if name in file_redirects.keys() else name for name in wiki_image_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6412de9f-8ad7-4c82-b49f-2599081fc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(row):\n",
    "    '''\n",
    "    Extract the details of a file\n",
    "    '''\n",
    "    text = row.revision.text._VALUE\n",
    "\n",
    "    categories = re.findall(categories_regex, text) if text else []\n",
    "    \n",
    "    # No way to do this with a list comprehension (nested conditions work only if there is always an else)\n",
    "    # Remap categories to their redirect and filter hidden categories\n",
    "    categories_nohidd = []\n",
    "    for category in categories:\n",
    "        category_norm = normalize_title(category)\n",
    "        if(category_norm not in hidden_categories):\n",
    "            if(category_norm in category_redirects.keys()):\n",
    "                if((c:=category_redirects[category_norm]) not in hidden_categories):\n",
    "                    categories_nohidd.append(c)\n",
    "            else:\n",
    "                categories_nohidd.append(category_norm)\n",
    "\n",
    "    return Row(\n",
    "        id=row.id,\n",
    "        title=normalize_title(row.title),\n",
    "        categories=categories_nohidd\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d08b05a6-7d88-4dfb-a937-2387898b8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of the processed files DataFrame\n",
    "schema_files = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                           StructField(\"title\", StringType(), True),\n",
    "                           StructField(\"categories\", ArrayType(StringType()), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db49e495-8be0-465e-8d87-9e9ca4f82ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also for files, we ignore redirects\n",
    "files = spark.createDataFrame(commons_files_raw.filter('redirect is null')\\\n",
    "                                        .rdd.map(extract_file).filter(lambda r: r is not None), \n",
    "                              schema=schema_files)\n",
    "# commons_files_raw.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71f1a788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "files.write.mode(\"overwrite\").parquet(FILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d237794",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = spark.read.parquet(FILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b67dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|       id|               title|          categories|\n",
      "+---------+--------------------+--------------------+\n",
      "|114792323|PATRICK MAGO (179...|[Players of Mount...|\n",
      "|114792324|RETURN TO REDFERN...|[Photographs by N...|\n",
      "|114792325|View from the Sev...|         [Arlingham]|\n",
      "|114792326|RETURN TO REDFERN...|[Photographs by N...|\n",
      "|114792328|RETURN TO REDFERN...|[Photographs by N...|\n",
      "|114792329| Marina Goliasse.jpg|                  []|\n",
      "|114792330|Downtown Ferndale...|                  []|\n",
      "|114792331|MATT PLACE (18392...|[Players of Mount...|\n",
      "|114792332|Benchmark on Citi...|[Richmond, North ...|\n",
      "|114792334|Portrait photogra...|  [Joséphin Péladan]|\n",
      "|114792335|SONNY BRISTOW (18...|[Players of Mount...|\n",
      "|114792336|The River Ayr - g...|[Ayr (civil paris...|\n",
      "|114792337|MATT PLACE (18370...|[Players of Mount...|\n",
      "|114792338|Path to Coton - g...|[Coton, Cambridge...|\n",
      "|114792339|MICHAEL MORRIS (1...|[Players of Mount...|\n",
      "|114792340|On top of A'Bhuid...|[Blair Atholl (ci...|\n",
      "|114792341|North Ave looking...|[Bank of America ...|\n",
      "|114792342|North Ave looking...|[Atlanta in the 2...|\n",
      "|114792343|Atlanta Women’s C...|[Atlanta Women's ...|\n",
      "|114792345|Woodruff Arts Cen...|[Woodruff Arts Ce...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257d802",
   "metadata": {},
   "source": [
    "## Categories/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "428a4d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# List of categories that appear in en.wikipedia\n",
    "categories_in_wikipedia = files.rdd.flatMap(lambda x: x.categories).distinct().map(Row(\"title\")).toDF()\n",
    "categories_in_wikipedia = categories_in_wikipedia.withColumn('in_en_wiki', lit(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbdec4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               title|in_en_wiki|\n",
      "+--------------------+----------+\n",
      "| Santon, Isle of Man|      true|\n",
      "|Christmas 2020 in...|      true|\n",
      "|Pacific Sogo Depa...|      true|\n",
      "|Players (men) by ...|      true|\n",
      "|Songs of the Beatles|      true|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_in_wikipedia.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef66f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190009"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_in_wikipedia.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "251f82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = categories.alias('c').join(categories_in_wikipedia, 'title', 'left').select('c.*', categories_in_wikipedia.in_en_wiki)\n",
    "categories = categories.na.fill(False, subset=[\"in_en_wiki\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "481d61c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.filter('in_en_wiki == True').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8214b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = '.'.join(CATEGORIES_PATH.split('.')[:-1]) + '-temp.parquet'\n",
    "categories.write.mode(\"overwrite\").parquet(temp_path)\n",
    "\n",
    "shutil.rmtree(CATEGORIES_PATH)\n",
    "os.rename(temp_path, CATEGORIES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3db6c0-c58e-4aac-89f7-c661bb1ead51",
   "metadata": {},
   "source": [
    "## Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b371038-b617-4b6c-93b9-caa9a61f82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04bf3c03e93e0e94a1d038aa36107c2a70cb015ab0da2208309d202be833925b"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
