{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6877c6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76c4818f-bded-4947-87c0-960bfe19575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/01 21:42:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import urllib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import wikitextprocessor as wtp\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "conf = pyspark.SparkConf().setMaster(\"local[10]\").setAll([\n",
    "                                   ('spark.jars.packages', 'com.databricks:spark-xml_2.12:0.8.0'),\n",
    "                                   ('spark.executor.memory', '4g'),\n",
    "                                   ('spark.driver.memory','5g'),\n",
    "                                   ('spark.driver.maxResultSize', '20G'),\n",
    "                                   ('spark.executor.heartbeatInterval', '60s'),\n",
    "                                   ('spark.network.timeout', '61s')\n",
    "                                  ])\n",
    "# create the session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb00d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.setLogLevel('DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3decbae-9d5d-44e5-8c4c-2f145f01b90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f970c2d1160>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e703d84c-8ad2-41cc-b56b-878e156247b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "COMMONS_DUMP = '/scratch/WikipediaImagesTaxonomy/dumps/commonswiki-20220220-pages-articles-multistream.xml.bz2'\n",
    "CATEGORIES_DUMP = '/scratch/WikipediaImagesTaxonomy/dumps/commonswiki-20220220-categories-multistream.xml.bz2'\n",
    "# COMMONS_DUMP_REDUCED = '../../commonswiki-20220220-pages-articles-multistream1.xml-p1p1500000.bz2'\n",
    "# COMMONS_DUMP_REDUCED = '../../commonswiki-20220220-pages-articles-multistream6.xml-p114543930p115400363.bz2'\n",
    "\n",
    "# Outputs (ideally in scratch/, but I don't have permissions)\n",
    "CATEGORIES_PATH = '/scratch/WikipediaImagesTaxonomy/commonswiki-20220220-category-network.parquet'\n",
    "FILES_PATH = '/scratch/WikipediaImagesTaxonomy/commonswiki-20220220-files.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bcb88d1b-3633-45c2-8b12-003f27ae5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/epfl-dlab/WikiPDA/blob/master/PaperAndCode/TopicsExtractionPipeline/GenerateDataframes.py\n",
    "def normalize_title(title, dumps=True):\n",
    "    \"\"\" Replace _ with space, remove anchor and namespace prefix, capitalize \"\"\"\n",
    "    title = urllib.parse.unquote(title)\n",
    "    if(dumps):\n",
    "        try:\n",
    "            title = title.split(':', 1)[1]\n",
    "        # Currently happens only for broken cross-namespace redirects\n",
    "        except IndexError:\n",
    "            return ''\n",
    "    title = title.strip()\n",
    "    if len(title) > 0:\n",
    "        title = title[0].upper() + title[1:]\n",
    "    n_title = title.replace(\"_\", \" \")\n",
    "    if '#' in n_title:\n",
    "        n_title = n_title.split('#')[0]\n",
    "    return n_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9e356-a201-4e56-af32-15d5908a6e40",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba4845f-bf35-4e09-9408-400b9e571eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "commons_categories_raw = spark.read.format('com.databricks.spark.xml') \\\n",
    "                                .options(rowTag='page').load(COMMONS_DUMP).filter(\"ns = '14'\")\n",
    "# commons_categories_raw.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b404d1e2-de8b-4aff-92e4-da006ea15a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- ns: long (nullable = true)\n",
      " |-- redirect: struct (nullable = true)\n",
      " |    |-- _VALUE: string (nullable = true)\n",
      " |    |-- _title: string (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _deleted: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _deleted: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- minor: string (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- parentid: long (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _bytes: long (nullable = true)\n",
      " |    |    |-- _xml:space: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commons_categories_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f5c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "commons_categories_raw.write.mode(\"overwrite\").parquet(CATEGORIES_DUMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "46d117ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "commons_categories_raw = spark.read.parquet(CATEGORIES_DUMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e7fac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a dictionary of redirects (old_title -> redirect_title)\n",
    "category_redirects = {normalize_title(r.title): normalize_title(r.redirect._title) \n",
    "                      for r in commons_categories_raw.filter('redirect is not null').collect()}\n",
    "len(category_redirects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ef081ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_regex = re.compile('(?<!wpTextbox1\\.value\\+=\\')(?<!wpTextbox1\\.value=\\')\\[\\[(Category:[^\\|]*?)(?:\\|.*?)*\\]\\]')\n",
    "hiddencat_regex = re.compile('__HIDDENCAT__' + \n",
    "                             '|\\{\\{[hH]iddencat\\}\\}' + \n",
    "                             '|\\{\\{[uU]ser category.*?\\}\\}' +\n",
    "                             '|\\{\\{[gG]lobal maintenance category\\}\\}' +\n",
    "                             '|\\[\\[(Category:Categories for discussion[^\\|]*?)(?:\\|.*?)*\\]\\]' +\n",
    "                             '|\\[\\[(Category:Media contributed by[^\\|]*?)(?:\\|.*?)*\\]\\]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "55b6d41d-d11d-4ff8-ad2c-ba7f2c431073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildsAccumulator(AccumulatorParam):\n",
    "    '''\n",
    "    Accumulator for childs: a dictionary mapping each category to its childs\n",
    "    '''\n",
    "    def zero(self, value):\n",
    "        return defaultdict(list)\n",
    "\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for key, value in val2.items():\n",
    "            val1[key] += value\n",
    "        return val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03bf2dad-11f8-4400-8304-7d767b297193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_category(row):\n",
    "    '''\n",
    "    Extract the details of a category\n",
    "    '''\n",
    "    title = normalize_title(row.title)\n",
    "    text = row.revision.text._VALUE\n",
    "        \n",
    "    parents = re.findall(categories_regex, text) if text else []\n",
    "    parents = [category_redirects[normalize_title(parent)] if normalize_title(parent) \n",
    "               in category_redirects.keys() else normalize_title(parent) for parent in parents]\n",
    "    global acc\n",
    "    if parents:\n",
    "        acc += {parent: [title] for parent in parents}\n",
    "    return Row(\n",
    "        id=row.id,\n",
    "        title=title,\n",
    "        parents=parents,\n",
    "        hiddencat=re.search(hiddencat_regex, text) is not None if text else False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "562fa86b-1ed3-4d36-84cc-76d3d16a79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of the processed categories DataFrame\n",
    "schema_cat = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                         StructField(\"title\", StringType(), True),\n",
    "                         StructField(\"parents\", ArrayType(StringType()), True),\n",
    "                         StructField(\"hiddencat\", BooleanType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd88f17e-9c3a-432a-bd73-8e10848623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ignore redirect categories, eventually remapping parents to their redirects\n",
    "acc = sc.accumulator(defaultdict(list), ChildsAccumulator())\n",
    "categories_clean = spark.createDataFrame(commons_categories_raw.filter('redirect is null')\\\n",
    "                                            .rdd.map(extract_category).filter(lambda r: r is not None), \n",
    "                                         schema=schema_cat)\n",
    "\n",
    "# commons_categories_raw.unpersist()\n",
    "# categories_clean.persist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9160c232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Workaround for the fact that the value of acc is used before it is filled\n",
    "\n",
    "TEMP_PATH = '../../dump.xml'\n",
    "\n",
    "categories_clean.write.format(\"com.databricks.spark.xml\").mode(\"overwrite\")\\\n",
    "                                         .options(rowTag='page', rootTag='pages').save(TEMP_PATH)\n",
    "\n",
    "# Remove files\n",
    "shutil.rmtree(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2ca709dc-e7d5-4391-98ea-6dbc6c69888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_childs = StructType([StructField('title', StringType(), True),\n",
    "                            StructField('childs', ArrayType(StringType(), True), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b54405a-86cc-4617-a770-75987e4f0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "childs_df = spark.createDataFrame(acc.value.items(), schema=schema_childs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1291a1a4-a05f-4480-b23a-24971a5f82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = categories_clean.alias('c').join(childs_df, categories_clean.title==childs_df.title, how='left').select('c.*', 'childs')\n",
    "# categories_clean.unpersist();\n",
    "# categories.persist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ee01b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/01 21:25:40 WARN TaskSetManager: Stage 7 contains a task of very large size (254960 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "categories.write.mode(\"overwrite\").parquet(CATEGORIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8dba6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = spark.read.parquet(CATEGORIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2104faf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+---------+------+\n",
      "|       id|               title|             parents|hiddencat|childs|\n",
      "+---------+--------------------+--------------------+---------+------+\n",
      "| 37673751|\"Bioagra\" plant i...|[Goświnowice, Fac...|    false|  null|\n",
      "|113462211|\"Dancing Dervishe...|[Kamal-ud-din Bih...|    false|  null|\n",
      "| 33938219|\"Dardanelles\", Po...|[Polkemmet collie...|    false|  null|\n",
      "| 12707820|\"Der Verwalter\", ...|[Buildings in Dor...|    false|  null|\n",
      "|112648762|\"Doktorhaus\" (Got...|[Cultural propert...|    false|  null|\n",
      "|102833104|\"Evaluarea impact...|[Photos from Parl...|    false|  null|\n",
      "| 66089937|\"Forever alive\" m...|                  []|    false|  null|\n",
      "| 89849682|\"Gracias\" rainbow...|[Rainbows, COVID-19]|    false|  null|\n",
      "| 18536700|\"Kniende\" (Karl T...|[Statues in Berli...|    false|  null|\n",
      "|109840454|     \"Krym\" roadster|[Roadsters by brand]|    false|  null|\n",
      "| 88168182|\"La Villa\", Schwe...|[Houses in Schwei...|    false|  null|\n",
      "|108271202|     \"Lewis Ferrero\"|[Finds Liaison Of...|    false|  null|\n",
      "|107451142|\"Los Amantes\" de ...|[Statues in Acapu...|    false|  null|\n",
      "| 83406893|\"Los árboles muer...|[Baltisky Dom The...|    false|  null|\n",
      "| 70246386|\"Partea Cneazului...|[Geological and p...|    false|  null|\n",
      "| 24295120|\"Philibert de l'O...|[\"Philibert de l'...|    false|  null|\n",
      "| 69676289|\"Propellers of th...|[Sculptures in Su...|    false|  null|\n",
      "| 52656265|\"The XII Sacred P...|[Photographs by A...|    false|  null|\n",
      "| 46870143|\"The treasure sta...|[1899 books from ...|    false|  null|\n",
      "| 62715557|\"Uncontrollably F...|[Uses of Wikidata...|    false|  null|\n",
      "+---------+--------------------+--------------------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "388964ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11029650"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8aaa645e-3028-4e98-a1da-0010cdbb4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_categories = categories.filter('hiddencat is True').select('title').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "105b43b1-da14-42e7-b0a3-71562e36f974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121711"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cba5ca-1e47-4740-beb3-7cd854dd3cde",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ae495bfe-99a2-4735-b105-cfc50d424494",
   "metadata": {},
   "outputs": [],
   "source": [
    "commons_files_raw = spark.read.format('com.databricks.spark.xml') \\\n",
    "                                .option(\"inferSchema\", \"false\")\\\n",
    "                                .schema(commons_categories_raw.schema)\\\n",
    "                                .options(rowTag='page').load(COMMONS_DUMP)\\\n",
    "                                .filter(\"ns = '6'\")\n",
    "# commons_files_raw.persist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9567b5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1903071"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a dictionary of redirects\n",
    "file_redirects = {normalize_title(r.title): normalize_title(r.redirect._title)\n",
    "                  for r in commons_files_raw.filter('redirect is not null').collect()}\n",
    "len(file_redirects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a698ca9-9a92-40ec-8d44-0148c12050f5",
   "metadata": {},
   "source": [
    "For now, we consider only the images that appear in en.wikipedia, discarding all the others. We can also ignore redirects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da5b5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of chunks of the WIT dataset\n",
    "WIT_DATASET = [f'/scratch/WIT_Dataset/wit_v1.train.all-0000{str(i)}-of-00010.tsv.gz' for i in np.arange(0, 10)]\n",
    "# WIT_DATASET = ['/scratch/WIT_Dataset/wit_v1.train.all-1percent_sample.tsv.gz']\n",
    "\n",
    "WIT_NAMES = '/scratch/WikipediaImagesTaxonomy/wit_names.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a214f82d-6ea7-4fc5-9bb6-7d8746af67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_image_names = []\n",
    "\n",
    "for chunk in WIT_DATASET:\n",
    "    wiki_image_names += pd.read_csv(chunk, sep=\"\\t\").query(\"language == 'en'\")\\\n",
    "                            .image_url.apply(lambda r: normalize_title(r.split('/')[-1], False)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21abc0dd-f651-444a-95c5-6c9bb7cde966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only unique values\n",
    "wiki_image_names = set(wiki_image_names)\n",
    "\n",
    "# Remap redirects\n",
    "wiki_image_names = {file_redirects[name] if name in file_redirects.keys() else name for name in wiki_image_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "620b0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WIT_NAMES, 'wb') as f:\n",
    "    pickle.dump(wiki_image_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bce5bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WIT_NAMES, 'rb') as f:\n",
    "    wiki_image_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "66aee0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3935543"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bd5167c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringType"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commons_files_raw.schema['title'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a78e42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_image_names_df = spark.createDataFrame(wiki_image_names, commons_files_raw.schema['title'].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d2653088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/01 21:44:35 WARN TaskSetManager: Stage 0 contains a task of very large size (15492 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3935543"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_image_names_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d436dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_title_udf = udf(lambda r: normalize_title(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c04f66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wit_files_raw = commons_files_raw.withColumn('title_norm', normalize_title_udf(commons_files_raw.title))\\\n",
    "                                 .join(wiki_image_names_df, col('title_norm') == wiki_image_names_df.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6412de9f-8ad7-4c82-b49f-2599081fc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(row):\n",
    "    '''\n",
    "    Extract the details of a file\n",
    "    '''\n",
    "    text = row.revision.text._VALUE\n",
    "\n",
    "    categories = re.findall(categories_regex, text) if text else []\n",
    "    \n",
    "    # No way to do this with a list comprehension (nested conditions work only if there is always an else)\n",
    "    # Remap categories to their redirect and filter hidden categories\n",
    "    categories_nohidd = []\n",
    "    for category in categories:\n",
    "        category_norm = normalize_title(category)\n",
    "        if(category_norm not in hidden_categories):\n",
    "            if(category_norm in category_redirects.keys()):\n",
    "                if((c:=category_redirects[category_norm]) not in hidden_categories):\n",
    "                    categories_nohidd.append(c)\n",
    "            else:\n",
    "                categories_nohidd.append(category_norm)\n",
    "\n",
    "    return Row(\n",
    "        id=row.id,\n",
    "        title=normalize_title(row.title),\n",
    "        categories=categories_nohidd\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d08b05a6-7d88-4dfb-a937-2387898b8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of the processed files DataFrame\n",
    "schema_files = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                           StructField(\"title\", StringType(), True),\n",
    "                           StructField(\"categories\", ArrayType(StringType()), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db49e495-8be0-465e-8d87-9e9ca4f82ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also for files, we ignore redirects\n",
    "files = spark.createDataFrame(wit_files_raw.filter('redirect is null')\\\n",
    "                                           .rdd.map(extract_file).filter(lambda r: r is not None), \n",
    "                              schema=schema_files)\n",
    "# commons_files_raw.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71f1a788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "files.write.mode(\"overwrite\").parquet(FILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d237794",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = spark.read.parquet(FILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b67dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|       id|               title|          categories|\n",
      "+---------+--------------------+--------------------+\n",
      "|114792323|PATRICK MAGO (179...|[Players of Mount...|\n",
      "|114792324|RETURN TO REDFERN...|[Photographs by N...|\n",
      "|114792325|View from the Sev...|         [Arlingham]|\n",
      "|114792326|RETURN TO REDFERN...|[Photographs by N...|\n",
      "|114792328|RETURN TO REDFERN...|[Photographs by N...|\n",
      "|114792329| Marina Goliasse.jpg|                  []|\n",
      "|114792330|Downtown Ferndale...|                  []|\n",
      "|114792331|MATT PLACE (18392...|[Players of Mount...|\n",
      "|114792332|Benchmark on Citi...|[Richmond, North ...|\n",
      "|114792334|Portrait photogra...|  [Joséphin Péladan]|\n",
      "|114792335|SONNY BRISTOW (18...|[Players of Mount...|\n",
      "|114792336|The River Ayr - g...|[Ayr (civil paris...|\n",
      "|114792337|MATT PLACE (18370...|[Players of Mount...|\n",
      "|114792338|Path to Coton - g...|[Coton, Cambridge...|\n",
      "|114792339|MICHAEL MORRIS (1...|[Players of Mount...|\n",
      "|114792340|On top of A'Bhuid...|[Blair Atholl (ci...|\n",
      "|114792341|North Ave looking...|[Bank of America ...|\n",
      "|114792342|North Ave looking...|[Atlanta in the 2...|\n",
      "|114792343|Atlanta Women’s C...|[Atlanta Women's ...|\n",
      "|114792345|Woodruff Arts Cen...|[Woodruff Arts Ce...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257d802",
   "metadata": {},
   "source": [
    "## Categories/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "428a4d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# List of categories that appear in en.wikipedia\n",
    "categories_in_wikipedia = files.rdd.flatMap(lambda x: x.categories).distinct().map(Row(\"title\")).toDF()\n",
    "categories_in_wikipedia = categories_in_wikipedia.withColumn('in_en_wiki', lit(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbdec4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               title|in_en_wiki|\n",
      "+--------------------+----------+\n",
      "| Santon, Isle of Man|      true|\n",
      "|Christmas 2020 in...|      true|\n",
      "|Pacific Sogo Depa...|      true|\n",
      "|Players (men) by ...|      true|\n",
      "|Songs of the Beatles|      true|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_in_wikipedia.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef66f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190009"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_in_wikipedia.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "251f82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = categories.alias('c').join(categories_in_wikipedia, 'title', 'left').select('c.*', categories_in_wikipedia.in_en_wiki)\n",
    "categories = categories.na.fill(False, subset=[\"in_en_wiki\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "481d61c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.filter('in_en_wiki == True').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8214b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = '.'.join(CATEGORIES_PATH.split('.')[:-1]) + '-temp.parquet'\n",
    "categories.write.mode(\"overwrite\").parquet(temp_path)\n",
    "\n",
    "shutil.rmtree(CATEGORIES_PATH)\n",
    "os.rename(temp_path, CATEGORIES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3db6c0-c58e-4aac-89f7-c661bb1ead51",
   "metadata": {},
   "source": [
    "## Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7b371038-b617-4b6c-93b9-caa9a61f82d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/01 21:42:28 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@490b6c5c rejected from java.util.concurrent.ThreadPoolExecutor@751468e9[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1299]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:815)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:791)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/01 21:42:28 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@5630fc9c rejected from java.util.concurrent.ThreadPoolExecutor@751468e9[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1299]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:815)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:791)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04bf3c03e93e0e94a1d038aa36107c2a70cb015ab0da2208309d202be833925b"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
