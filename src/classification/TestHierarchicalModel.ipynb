{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import help_functions as hf\n",
    "import numpy as np\n",
    "import time\n",
    "hf.setup_gpu(gpu_nr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 validated image filenames belonging to 31 classes.\n",
      "Found 50000 validated image filenames belonging to 31 classes.\n"
     ]
    }
   ],
   "source": [
    "config_nr = 109\n",
    "with open('training_configurations.json', 'r') as fp:\n",
    "    config = json.load(fp)[str(config_nr)]\n",
    "\n",
    "train, _ = hf.get_flow(df_file=config['data_folder'] + '/test_df.json.bz2',\n",
    "                       nr_classes=config['nr_classes'],\n",
    "                       batch_size=config['batch_size'],\n",
    "                       image_dimension=config['image_dimension'])\n",
    "\n",
    "val, _ = hf.get_flow(df_file=config['data_folder'] + '/val_stop_df.json.bz2',\n",
    "                     nr_classes=config['nr_classes'],\n",
    "                     batch_size=config['batch_size'],\n",
    "                     image_dimension=config['image_dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Animals': 0,\n",
       " 'Architecture': 1,\n",
       " 'Art': 2,\n",
       " 'Belief': 3,\n",
       " 'Biology': 4,\n",
       " 'Chemistry': 5,\n",
       " 'Culture': 6,\n",
       " 'Earth & Environment': 7,\n",
       " 'Entertainment': 8,\n",
       " 'Events': 9,\n",
       " 'Food': 10,\n",
       " 'Fossils': 11,\n",
       " 'History': 12,\n",
       " 'Landscapes': 13,\n",
       " 'Literature': 14,\n",
       " 'Maps & Flags': 15,\n",
       " 'Mathematics': 16,\n",
       " 'Medicine & Health': 17,\n",
       " 'Music': 18,\n",
       " 'Nature': 19,\n",
       " 'People': 20,\n",
       " 'Physics': 21,\n",
       " 'Places': 22,\n",
       " 'Plants': 23,\n",
       " 'Politics': 24,\n",
       " 'STEM': 25,\n",
       " 'Society': 26,\n",
       " 'Space': 27,\n",
       " 'Sports': 28,\n",
       " 'Technology & Engineering': 29,\n",
       " 'Transportation': 30}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_ij = 1 if the class j is a subclass of the class i. I.e.: row i, column j is 1.\n",
    "# - note that if M_ij is 1 then M_ji is necessarily 0 as the subclass relation is not reflexive\n",
    "# - note that M_ij is also 1 if j is not a direct descendant of i (as it is defined in Giunchiglia's model)\n",
    "\n",
    "\n",
    "#                 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n",
    "mask = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 0 Animals\n",
    "                 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 1 Architecture\n",
    "                 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 2 Art\n",
    "                 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 3 Belief\n",
    "                 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 4 Biology\n",
    "                 [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 5 Chemistry\n",
    "                 [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 6 Culture\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 7 Earth & Environment\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 8 Entertainment\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 9 Events\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 10 Food\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 11 Fossils\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 12 History\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 13 Landscapes\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 14 Literature\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 15 Maps & Flags\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 16 Mathematics\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 17 Medicine & Health\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 18 Music\n",
    "                 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], # 19 Nature\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 20 People\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 21 Physics\n",
    "                 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], # 22 Places\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], # 23 Plants\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], # 24 Politics\n",
    "                 [1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], # 25 STEM\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1], # 26 Society\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], # 27 Space\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], # 28 Sports\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], # 29 Technology & Engineering\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], # 30 Transportation\n",
    "                 ], dtype=np.float32)\n",
    "\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# G = nx.from_numpy_matrix(mask)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# label_dict = {v: k for k, v in train.class_indices.items()}\n",
    "# nx.draw(G, labels=label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 1\n",
    "Trying by creating a model class (which is necessary as we want different behaviours when training and when predicting) and then running model.fit. Works, but the loss in model.fit is 0.000. Weird\n",
    "\n",
    "Inspired in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "accuracy = tf.keras.metrics.Accuracy(name='accuracy')\n",
    "pr_auc = tf.keras.metrics.AUC(num_thresholds=50, curve='PR', name='pr_auc', multi_label=True)\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class HierarchicalModel(keras.Model):\n",
    "\n",
    "    def __init__(self, nr_labels, adj_matrix):\n",
    "        super(HierarchicalModel, self).__init__()\n",
    "        self.nr_labels = nr_labels\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.basemodel = EfficientNetB2(include_top=False, \n",
    "                                        weights=None, \n",
    "                                        classes=nr_labels, \n",
    "                                        input_shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(128, activation='relu')\n",
    "        self.dense_final = layers.Dense(self.nr_labels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.basemodel(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        outputs = self.dense_final(x)\n",
    "        if training:\n",
    "            return outputs\n",
    "        else:\n",
    "            return self.max_constrain(outputs)\n",
    "\n",
    "    def max_constrain(self, output):\n",
    "        \"\"\"\n",
    "        Constrains the output given the hierarchy expressed by the mask.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        output = tf.expand_dims(output, axis=1) # Pytorch unsqueeze()\n",
    "        batch_output = tf.broadcast_to(output,            [batch_size, self.nr_labels, self.nr_labels]) # this is H in the MCM equation\n",
    "        batch_mask   = tf.broadcast_to(self.adj_matrix,   [batch_size, self.nr_labels, self.nr_labels])\n",
    "        constrained_output = tf.math.reduce_max(batch_output * batch_mask, axis=2)\n",
    "        return constrained_output\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Applies the hierarchy constraint module, calculates the loss function value,\n",
    "        computes the gradients and optimizes the weights.\n",
    "        \"\"\"\n",
    "        inputs, labels = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_probs = self.call(inputs, training=True)\n",
    "            y_probs_constrained = (1 - labels) * self.max_constrain(y_probs) + \\\n",
    "                                    labels * self.max_constrain(labels * y_probs)\n",
    "            loss_value = bce(labels, y_probs_constrained)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        grads = tape.gradient(loss_value, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        loss_tracker.update_state(loss_value)\n",
    "        pr_auc.update_state(labels, y_probs)\n",
    "        \n",
    "        return {'loss': loss_tracker.result(), 'pr_auc': pr_auc.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker, pr_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "30/30 [==============================] - 89s 3s/step - loss: 0.2834 - pr_auc: 0.0766 - val_loss: 0.0000e+00 - val_pr_auc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "30/30 [==============================] - 75s 3s/step - loss: 0.1843 - pr_auc: 0.0775 - val_loss: 0.0000e+00 - val_pr_auc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model = HierarchicalModel(nr_labels=len(train.class_indices), adj_matrix=mask)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "history = model.fit(train, epochs=2, steps_per_epoch=30, validation_steps=5, validation_data=val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.283385694026947, 0.18430249392986298],\n",
       " 'pr_auc': [0.07655449211597443, 0.07747188955545425],\n",
       " 'val_loss': [0.0, 0.0],\n",
       " 'val_pr_auc': [0.0, 0.0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 2\n",
    "This works but doesn't give the expected behaviour since training and predicting need to have different behaviours. If we do it this way then we can't control what happens when calling model(input).\n",
    "\n",
    "Inspired in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Define model\n",
    "nr_labels = len(train.class_indices)\n",
    "basemodel = EfficientNetB2(include_top=False, weights=None, classes=nr_labels, input_shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "inputs = Input(shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "x = basemodel(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(nr_labels, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Training functions\n",
    "@tf.function\n",
    "def max_constrain(output, mask):\n",
    "    \"\"\"\n",
    "    Constrains the output given the hierarchy expressed by the mask.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(output) # or output.shape[0]\n",
    "\n",
    "    output = tf.expand_dims(output, axis=1) # Pytorch unsqueeze()\n",
    "\n",
    "    batch_output = tf.broadcast_to(output, [batch_size, nr_labels, nr_labels]) # this is H in the MCM equation\n",
    "    batch_mask   = tf.broadcast_to(mask,   [batch_size, nr_labels, nr_labels])\n",
    "\n",
    "    constrained_output = tf.math.reduce_max(batch_output * batch_mask, axis=2)\n",
    "\n",
    "    return constrained_output\n",
    "\n",
    "@tf.function   # to speed up: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction\n",
    "def train_step(inputs, labels, mask):\n",
    "    \"\"\"\n",
    "    Applies the hierarchy constraint module, calculates the loss function value,\n",
    "    computes the gradients and optimizes the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f'INPUTS: \\n{inputs}')\n",
    "    # print(f'LABELS: \\n{labels}')\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_probs = model(inputs, training=True)\n",
    "\n",
    "        # Extra steps for coherent HMC:\n",
    "        # 1. max constraint module        \n",
    "        # term_1 = (1 - labels) * max_constrain(y_probs, mask)\n",
    "        # term_2 = labels * max_constrain(labels * y_probs, mask)\n",
    "        # y_probs_constrained = term_1 + term_2\n",
    "        y_probs_constrained = (1 - labels) * max_constrain(y_probs, mask) + labels * max_constrain(labels * y_probs, mask)\n",
    "\n",
    "        # 2. modify what is sent to binary cross-entropy function\n",
    "        loss_value = model.compiled_loss(labels, y_probs_constrained)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    \n",
    "    return loss_value, grads\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------- Training loop \n",
    "epochs = 2\n",
    "steps_per_epoch_train = len(train.classes) // config['batch_size'] + 1\n",
    "steps_per_epoch_val = len(val.classes) // config['batch_size'] + 1\n",
    "print(f'steps_per_epoch_train: {steps_per_epoch_train}.')\n",
    "print(f'steps_per_epoch_val: {steps_per_epoch_val}.')\n",
    "mask_tf = tf.convert_to_tensor(mask)\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}\\n')\n",
    "    end = time.time()\n",
    "\n",
    "    train_loss = tf.metrics.Mean('train_loss')\n",
    "    train_acc = tf.metrics.CategoricalAccuracy('train_accuracy')\n",
    "    val_loss = tf.metrics.Mean('val_loss')\n",
    "    val_acc = tf.metrics.CategoricalAccuracy('val_accuracy')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train):\n",
    "        if step >= 22:\n",
    "        # if step >= steps_per_epoch_train:\n",
    "            break\n",
    "        # Calculate loss and gradients, then optimize parameters\n",
    "        loss, grads = train_step(x_batch_train, y_batch_train, mask_tf)\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        train_loss.update_state(loss)\n",
    "        train_acc.update_state(y_batch_train, model(x_batch_train, training=True))\n",
    "\n",
    "        # Progress logging\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Training loss (for one batch) at step {step}: {float(loss):.4f}\")\n",
    "            print(f\"Seen so far: {(step + 1) * config['batch_size']} samples\")\n",
    "            temp_end = time.time()\n",
    "            print(f'time: {np.round((temp_end-end) / 60, 2)} minutes')\n",
    "            end = time.time()      \n",
    "        \n",
    "\n",
    "    # Evaluate on validation set\n",
    "    print('Evaluating on validation set...')\n",
    "    for step, (x_batch_val, y_batch_val) in enumerate(val):\n",
    "        if step >= 22:\n",
    "        # if step >= steps_per_epoch_val:\n",
    "            break\n",
    "        loss, _ = train_step(x_batch_val, y_batch_val, mask_tf)\n",
    "        val_loss.update_state(loss)\n",
    "        val_acc.update_state(y_batch_val, model(x_batch_val, training=False))\n",
    "    \n",
    "    train_losses.append(train_loss.result().numpy())\n",
    "    train_accs.append(train_acc.result().numpy())\n",
    "    val_losses.append(val_loss.result().numpy())\n",
    "    val_accs.append(val_acc.result().numpy())\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Accuracy: {100*train_accs[-1]:.2f}%, Val Loss: {val_losses[-1]:.4f}, Val Accuracy: {100*val_accs[-1]:.2f}%')\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'epoch': list(range(0, epochs)), \n",
    "     'accuracy': train_accs, \n",
    "     'loss': train_losses, \n",
    "     'val_accuracy': val_accs, \n",
    "     'val_loss': val_losses}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests of the Maximum Constraint Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test of maximum constraint module: 3 classes\n",
    "# Class 1 is the parent of classes 2 and 3\n",
    "mask = np.array([[1, 0, 0],\n",
    "                 [1, 1, 0],\n",
    "                 [1, 0, 1]], dtype=np.float32)\n",
    "np.fill_diagonal(mask, 1)\n",
    "mask = np.transpose(mask)\n",
    "\n",
    "# Case 1: the parent has greater probability than both children. Nothing changes.\n",
    "probs = np.array([[0.7, 0.5, 0.2]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == probs).all())\n",
    "\n",
    "# Case 2: the parent has smaller probability than child class 2. \n",
    "probs = np.array([[0.1, 0.5, 0.2]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.5, 0.5, 0.2], dtype=np.float32)).all())\n",
    "\n",
    "# Case 3: the parent has smaller probability than child class 3. \n",
    "probs = np.array([[0.1, 0.3, 0.8]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.8, 0.3, 0.8], dtype=np.float32)).all())\n",
    "\n",
    "# Case 4: the parent has smaller probability than both children. \n",
    "probs = np.array([[0.2, 0.3, 0.5]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.5, 0.3, 0.5], dtype=np.float32)).all())\n",
    "\n",
    "# Unit test of maximum constraint module: 5 classes\n",
    "#     1  2\n",
    "#    / \\  \\\n",
    "#   3   4  5\n",
    "# Class 1 is the parent of classes 3 and 4; class 2 is the parent of class 5\n",
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "# Case 1: all is good (1 has great prob than 3 and 4; 2 has greater prob than 5)\n",
    "probs = np.array([[0.7, 0.5, 0.2, 0.1, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == probs).all())\n",
    "\n",
    "# Case 2: 1 has smaller prob than 3; 2 is ok\n",
    "probs = np.array([[0.7, 0.5, 0.9, 0.1, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.5, 0.9, 0.1, 0.1], dtype=np.float32)).all())\n",
    "\n",
    "# Case 3: 1 has smaller prob than 4; 2 is ok\n",
    "probs = np.array([[0.7, 0.5, 0.6, 0.9, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.5, 0.6, 0.9, 0.1], dtype=np.float32)).all())\n",
    "\n",
    "# Case 4: 1 has smaller prob than 4; 2 has smaller prob than 5\n",
    "probs = np.array([[0.7, 0.5, 0.6, 0.9, 0.8]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.8, 0.6, 0.9, 0.8], dtype=np.float32)).all())\n",
    "\n",
    "def get_parent_child_dict(mask):\n",
    "    dict = {}\n",
    "    for row_idx in range(mask.shape[0]):\n",
    "        children_idx = np.nonzero(mask[row_idx, :])[0]\n",
    "        if children_idx.size:\n",
    "            dict[row_idx] = children_idx\n",
    "    return dict\n",
    "\n",
    "def is_coherent(probs, parent_idx, children_idx):\n",
    "    for child_idx in children_idx:\n",
    "        assert(probs[parent_idx] >= probs[child_idx])\n",
    "\n",
    "def coherence_test(mask, nr_tests=100):\n",
    "    parent_child_dict = get_parent_child_dict(mask)\n",
    "    for _ in range(nr_tests):\n",
    "        probs = np.random.uniform(low=0.0, high=1.0, size=(1, mask.shape[0]))\n",
    "        probs = np.array(probs, dtype=np.float32)\n",
    "        coherent_probs = max_constrain(probs, mask)[0] # '0' because the output of max_constrain is a copy of the same coherent probs in several lines\n",
    "        for parent_idx in parent_child_dict:\n",
    "            is_coherent(coherent_probs, parent_idx, parent_child_dict[parent_idx])  \n",
    "    print('Test passed, all probs are coherent!')\n",
    "\n",
    "# -------------------- Tress with 2 layers\n",
    "\n",
    "#   1   2\n",
    "#  / \\   \\\n",
    "# 3   4   5\n",
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask)    \n",
    "\n",
    "#     1\n",
    "#  / / \\ \\\n",
    "# 2 3  4  5\n",
    "mask = np.array([[1, 1, 1, 1, 1],\n",
    "                 [0, 1, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask)   \n",
    "\n",
    "#  1        5     8 \n",
    "# / \\ \\    / \\   / \\\n",
    "# 2 3  4  6   7  9  10\n",
    "mask = np.array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask) \n",
    "\n",
    "# -------------------- Tress with 3 or more layers\n",
    "\n",
    "# 1\n",
    "# |\n",
    "# 2\n",
    "# |\n",
    "# 3\n",
    "mask = np.array([[1, 1, 1],\n",
    "                 [0, 1, 1],\n",
    "                 [0, 0, 1]], dtype=np.float32)\n",
    "coherence_test(mask)\n",
    "\n",
    "#     1    5\n",
    "#    / \\   |\n",
    "#   2   3  6\n",
    "#  /       |\n",
    "# 4        7\n",
    "mask = np.array([[1, 1, 1, 1, 0, 0, 0],\n",
    "                 [0, 1, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "coherence_test(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests of the loss function\n",
    "Compare the result with the official PyTorch implementation. Indeed the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836884"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "                 \n",
    "# y_probs = np.array([[0.35, 0.03, 0.6, 0.08, 0.1],\n",
    "#                     [0.9, 0.03, 0.05, 0.98, 0.01],\n",
    "#                     [0.57, 0.33, 0.7, 0.2, 0.12]], dtype=np.float32)\n",
    "\n",
    "y_probs = np.array([[0.35, 0.03, 0.6, 0.08, 0.1]], dtype=np.float32)\n",
    "\n",
    "labels = np.array([[0, 0, 1, 0, 1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "term_1 = (1 - labels) * max_constrain(y_probs, mask)\n",
    "term_2 = labels * max_constrain(labels * y_probs, mask)\n",
    "y_probs_constrained = term_1 + term_2\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce(labels, y_probs_constrained).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7f323a1c2c8b3d38dc94a01188981c510c9b5df10e2cc2d7fa4f2b45d318cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
