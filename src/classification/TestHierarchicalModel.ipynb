{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import help_functions as hf\n",
    "import numpy as np\n",
    "import time\n",
    "hf.setup_gpu(gpu_nr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 validated image filenames belonging to 31 classes.\n",
      "Found 50000 validated image filenames belonging to 31 classes.\n"
     ]
    }
   ],
   "source": [
    "config_nr = 109\n",
    "with open('training_configurations.json', 'r') as fp:\n",
    "    config = json.load(fp)[str(config_nr)]\n",
    "\n",
    "train, _ = hf.get_flow(df_file=config['data_folder'] + '/test_df.json.bz2',\n",
    "                       nr_classes=config['nr_classes'],\n",
    "                       batch_size=config['batch_size'],\n",
    "                       image_dimension=config['image_dimension'])\n",
    "\n",
    "val, _ = hf.get_flow(df_file=config['data_folder'] + '/val_stop_df.json.bz2',\n",
    "                     nr_classes=config['nr_classes'],\n",
    "                     batch_size=config['batch_size'],\n",
    "                     image_dimension=config['image_dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Animals': 0,\n",
       " 'Architecture': 1,\n",
       " 'Art': 2,\n",
       " 'Belief': 3,\n",
       " 'Biology': 4,\n",
       " 'Chemistry': 5,\n",
       " 'Culture': 6,\n",
       " 'Earth & Environment': 7,\n",
       " 'Entertainment': 8,\n",
       " 'Events': 9,\n",
       " 'Food': 10,\n",
       " 'Fossils': 11,\n",
       " 'History': 12,\n",
       " 'Landscapes': 13,\n",
       " 'Literature': 14,\n",
       " 'Maps & Flags': 15,\n",
       " 'Mathematics': 16,\n",
       " 'Medicine & Health': 17,\n",
       " 'Music': 18,\n",
       " 'Nature': 19,\n",
       " 'People': 20,\n",
       " 'Physics': 21,\n",
       " 'Places': 22,\n",
       " 'Plants': 23,\n",
       " 'Politics': 24,\n",
       " 'STEM': 25,\n",
       " 'Society': 26,\n",
       " 'Space': 27,\n",
       " 'Sports': 28,\n",
       " 'Technology & Engineering': 29,\n",
       " 'Transportation': 30}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_ij = 1 if the class j is a subclass of the class i. I.e.: row i, column j is 1.\n",
    "# - note that if M_ij is 1 then M_ji is necessarily 0 as the subclass relation is not reflexive\n",
    "# - note that M_ij is also 1 if j is not a direct descendant of i \n",
    "\n",
    "\n",
    "#                 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n",
    "mask = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 0 Animals\n",
    "                 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 1 Architecture\n",
    "                 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 2 Art\n",
    "                 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 3 Belief\n",
    "                 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 4 Biology\n",
    "                 [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 5 Chemistry\n",
    "                 [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 6 Culture\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 7 Earth & Environment\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 8 Entertainment\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 9 Events\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 10 Food\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 11 Fossils\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 12 History\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 13 Landscapes\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 14 Literature\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 15 Maps & Flags\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 16 Mathematics\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 17 Medicine & Health\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 18 Music\n",
    "                 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], # 19 Nature\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 20 People\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 21 Physics\n",
    "                 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], # 22 Places\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], # 23 Plants\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], # 24 Politics\n",
    "                 [1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], # 25 STEM\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1], # 26 Society\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], # 27 Space\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], # 28 Sports\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], # 29 Technology & Engineering\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], # 30 Transportation\n",
    "                 ], dtype=np.float32)\n",
    "\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# G = nx.from_numpy_matrix(mask)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# label_dict = {v: k for k, v in train.class_indices.items()}\n",
    "# nx.draw(G, labels=label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 1\n",
    "Trying by creating a model class (which is necessary as we want different behaviours when training and when predicting) and then running model.fit. Works, but the loss in model.fit is 0.000. Weird\n",
    "\n",
    "Inspired in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "m = tf.keras.metrics.Accuracy()\n",
    "\n",
    "class HierarchicalModel(keras.Model):\n",
    "\n",
    "    def __init__(self, nr_labels, adj_matrix):\n",
    "        super(HierarchicalModel, self).__init__()\n",
    "        self.nr_labels = nr_labels\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.basemodel = EfficientNetB2(include_top=False, \n",
    "                                        weights=None, \n",
    "                                        classes=nr_labels, \n",
    "                                        input_shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(128, activation='relu')\n",
    "        self.dense_final = layers.Dense(self.nr_labels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.basemodel(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        outputs = self.dense_final(x)\n",
    "        if training:\n",
    "            return outputs\n",
    "        else:\n",
    "            # return outputs\n",
    "            return self.max_constrain(outputs)\n",
    "\n",
    "    def max_constrain(self, output):\n",
    "        \"\"\"\n",
    "        Constrains the output given the hierarchy expressed by the mask.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        output = tf.expand_dims(output, axis=1) # Pytorch unsqueeze()\n",
    "        batch_output = tf.broadcast_to(output,            [batch_size, self.nr_labels, self.nr_labels]) # this is H in the MCM equation\n",
    "        batch_mask   = tf.broadcast_to(self.adj_matrix,   [batch_size, self.nr_labels, self.nr_labels])\n",
    "        constrained_output = tf.math.reduce_max(batch_output * batch_mask, axis=2)\n",
    "        return constrained_output\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Applies the hierarchy constraint module, calculates the loss function value,\n",
    "        computes the gradients and optimizes the weights.\n",
    "        \"\"\"\n",
    "        inputs, labels = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_probs = self.call(inputs, training=True)\n",
    "            y_probs_constrained = (1 - labels) * self.max_constrain(y_probs) + \\\n",
    "                                    labels * self.max_constrain(labels * y_probs)\n",
    "            loss_value = self.compiled_loss(labels, y_probs_constrained)\n",
    "            loss_value = bce(labels, y_probs_constrained)\n",
    "            loss_value = bce(labels, y_probs)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        grads = tape.gradient(loss_value, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(labels, y_probs)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 3/98 [..............................] - ETA: 8:11 - loss: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-6597636265a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierarchicalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnr_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = HierarchicalModel(nr_labels=len(train.class_indices), adj_matrix=mask)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "model.fit(train, epochs=2, validation_data=val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 2\n",
    "This works but doesn't give the expected behaviour since training and predicting need to have different behaviours. If we do it this way then we can't control what happens when calling model(input).\n",
    "\n",
    "Inspired in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Define model\n",
    "nr_labels = len(train.class_indices)\n",
    "basemodel = EfficientNetB2(include_top=False, weights=None, classes=nr_labels, input_shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "inputs = Input(shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "x = basemodel(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(nr_labels, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Training functions\n",
    "@tf.function\n",
    "def max_constrain(output, mask):\n",
    "    \"\"\"\n",
    "    Constrains the output given the hierarchy expressed by the mask.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(output) # or output.shape[0]\n",
    "\n",
    "    output = tf.expand_dims(output, axis=1) # Pytorch unsqueeze()\n",
    "\n",
    "    batch_output = tf.broadcast_to(output, [batch_size, nr_labels, nr_labels]) # this is H in the MCM equation\n",
    "    batch_mask   = tf.broadcast_to(mask,   [batch_size, nr_labels, nr_labels])\n",
    "\n",
    "    constrained_output = tf.math.reduce_max(batch_output * batch_mask, axis=2)\n",
    "\n",
    "    return constrained_output\n",
    "\n",
    "@tf.function   # to speed up: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction\n",
    "def train_step(inputs, labels, mask):\n",
    "    \"\"\"\n",
    "    Applies the hierarchy constraint module, calculates the loss function value,\n",
    "    computes the gradients and optimizes the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f'INPUTS: \\n{inputs}')\n",
    "    # print(f'LABELS: \\n{labels}')\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_probs = model(inputs, training=True)\n",
    "\n",
    "        # Extra steps for coherent HMC:\n",
    "        # 1. max constraint module        \n",
    "        # term_1 = (1 - labels) * max_constrain(y_probs, mask)\n",
    "        # term_2 = labels * max_constrain(labels * y_probs, mask)\n",
    "        # y_probs_constrained = term_1 + term_2\n",
    "        y_probs_constrained = (1 - labels) * max_constrain(y_probs, mask) + labels * max_constrain(labels * y_probs, mask)\n",
    "\n",
    "        # 2. modify what is sent to binary cross-entropy function\n",
    "        loss_value = model.compiled_loss(labels, y_probs_constrained)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    \n",
    "    return loss_value, grads\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------- Training loop \n",
    "epochs = 2\n",
    "steps_per_epoch_train = len(train.classes) // config['batch_size'] + 1\n",
    "steps_per_epoch_val = len(val.classes) // config['batch_size'] + 1\n",
    "print(f'steps_per_epoch_train: {steps_per_epoch_train}.')\n",
    "print(f'steps_per_epoch_val: {steps_per_epoch_val}.')\n",
    "mask_tf = tf.convert_to_tensor(mask)\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}\\n')\n",
    "    end = time.time()\n",
    "\n",
    "    train_loss = tf.metrics.Mean('train_loss')\n",
    "    train_acc = tf.metrics.CategoricalAccuracy('train_accuracy')\n",
    "    val_loss = tf.metrics.Mean('val_loss')\n",
    "    val_acc = tf.metrics.CategoricalAccuracy('val_accuracy')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train):\n",
    "        if step >= 22:\n",
    "        # if step >= steps_per_epoch_train:\n",
    "            break\n",
    "        # Calculate loss and gradients, then optimize parameters\n",
    "        loss, grads = train_step(x_batch_train, y_batch_train, mask_tf)\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        train_loss.update_state(loss)\n",
    "        train_acc.update_state(y_batch_train, model(x_batch_train, training=True))\n",
    "\n",
    "        # Progress logging\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Training loss (for one batch) at step {step}: {float(loss):.4f}\")\n",
    "            print(f\"Seen so far: {(step + 1) * config['batch_size']} samples\")\n",
    "            temp_end = time.time()\n",
    "            print(f'time: {np.round((temp_end-end) / 60, 2)} minutes')\n",
    "            end = time.time()      \n",
    "        \n",
    "\n",
    "    # Evaluate on validation set\n",
    "    print('Evaluating on validation set...')\n",
    "    for step, (x_batch_val, y_batch_val) in enumerate(val):\n",
    "        if step >= 22:\n",
    "        # if step >= steps_per_epoch_val:\n",
    "            break\n",
    "        loss, _ = train_step(x_batch_val, y_batch_val, mask_tf)\n",
    "        val_loss.update_state(loss)\n",
    "        val_acc.update_state(y_batch_val, model(x_batch_val, training=False))\n",
    "    \n",
    "    train_losses.append(train_loss.result().numpy())\n",
    "    train_accs.append(train_acc.result().numpy())\n",
    "    val_losses.append(val_loss.result().numpy())\n",
    "    val_accs.append(val_acc.result().numpy())\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss: {train_losses[-1]:.4f}, Accuracy: {100*train_accs[-1]:.2f}%, Val Loss: {val_losses[-1]:.4f}, Val Accuracy: {100*val_accs[-1]:.2f}%')\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'epoch': list(range(0, epochs)), \n",
    "     'accuracy': train_accs, \n",
    "     'loss': train_losses, \n",
    "     'val_accuracy': val_accs, \n",
    "     'val_loss': val_losses}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('history.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests of the Maximum Constraint Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test of maximum constraint module: 3 classes\n",
    "# Class 1 is the parent of classes 2 and 3\n",
    "mask = np.array([[1, 0, 0],\n",
    "                 [1, 1, 0],\n",
    "                 [1, 0, 1]], dtype=np.float32)\n",
    "np.fill_diagonal(mask, 1)\n",
    "mask = np.transpose(mask)\n",
    "\n",
    "# Case 1: the parent has greater probability than both children. Nothing changes.\n",
    "probs = np.array([[0.7, 0.5, 0.2]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == probs).all())\n",
    "\n",
    "# Case 2: the parent has smaller probability than child class 2. \n",
    "probs = np.array([[0.1, 0.5, 0.2]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.5, 0.5, 0.2], dtype=np.float32)).all())\n",
    "\n",
    "# Case 3: the parent has smaller probability than child class 3. \n",
    "probs = np.array([[0.1, 0.3, 0.8]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.8, 0.3, 0.8], dtype=np.float32)).all())\n",
    "\n",
    "# Case 4: the parent has smaller probability than both children. \n",
    "probs = np.array([[0.2, 0.3, 0.5]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.5, 0.3, 0.5], dtype=np.float32)).all())\n",
    "\n",
    "# Unit test of maximum constraint module: 5 classes\n",
    "#     1  2\n",
    "#    / \\  \\\n",
    "#   3   4  5\n",
    "# Class 1 is the parent of classes 3 and 4; class 2 is the parent of class 5\n",
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "# Case 1: all is good (1 has great prob than 3 and 4; 2 has greater prob than 5)\n",
    "probs = np.array([[0.7, 0.5, 0.2, 0.1, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == probs).all())\n",
    "\n",
    "# Case 2: 1 has smaller prob than 3; 2 is ok\n",
    "probs = np.array([[0.7, 0.5, 0.9, 0.1, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.5, 0.9, 0.1, 0.1], dtype=np.float32)).all())\n",
    "\n",
    "# Case 3: 1 has smaller prob than 4; 2 is ok\n",
    "probs = np.array([[0.7, 0.5, 0.6, 0.9, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.5, 0.6, 0.9, 0.1], dtype=np.float32)).all())\n",
    "\n",
    "# Case 4: 1 has smaller prob than 4; 2 has smaller prob than 5\n",
    "probs = np.array([[0.7, 0.5, 0.6, 0.9, 0.8]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.8, 0.6, 0.9, 0.8], dtype=np.float32)).all())\n",
    "\n",
    "def get_parent_child_dict(mask):\n",
    "    dict = {}\n",
    "    for row_idx in range(mask.shape[0]):\n",
    "        children_idx = np.nonzero(mask[row_idx, :])[0]\n",
    "        if children_idx.size:\n",
    "            dict[row_idx] = children_idx\n",
    "    return dict\n",
    "\n",
    "def is_coherent(probs, parent_idx, children_idx):\n",
    "    for child_idx in children_idx:\n",
    "        assert(probs[parent_idx] >= probs[child_idx])\n",
    "\n",
    "def coherence_test(mask, nr_tests=100):\n",
    "    parent_child_dict = get_parent_child_dict(mask)\n",
    "    for _ in range(nr_tests):\n",
    "        probs = np.random.uniform(low=0.0, high=1.0, size=(1, mask.shape[0]))\n",
    "        probs = np.array(probs, dtype=np.float32)\n",
    "        coherent_probs = max_constrain(probs, mask)[0] # '0' because the output of max_constrain is a copy of the same coherent probs in several lines\n",
    "        for parent_idx in parent_child_dict:\n",
    "            is_coherent(coherent_probs, parent_idx, parent_child_dict[parent_idx])  \n",
    "    print('Test passed, all probs are coherent!')\n",
    "\n",
    "# -------------------- Tress with 2 layers\n",
    "\n",
    "#   1   2\n",
    "#  / \\   \\\n",
    "# 3   4   5\n",
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask)    \n",
    "\n",
    "#     1\n",
    "#  / / \\ \\\n",
    "# 2 3  4  5\n",
    "mask = np.array([[1, 1, 1, 1, 1],\n",
    "                 [0, 1, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask)   \n",
    "\n",
    "#  1        5     8 \n",
    "# / \\ \\    / \\   / \\\n",
    "# 2 3  4  6   7  9  10\n",
    "mask = np.array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask) \n",
    "\n",
    "# -------------------- Tress with 3 or more layers\n",
    "\n",
    "# 1\n",
    "# |\n",
    "# 2\n",
    "# |\n",
    "# 3\n",
    "mask = np.array([[1, 1, 1],\n",
    "                 [0, 1, 1],\n",
    "                 [0, 0, 1]], dtype=np.float32)\n",
    "coherence_test(mask)\n",
    "\n",
    "#     1    5\n",
    "#    / \\   |\n",
    "#   2   3  6\n",
    "#  /       |\n",
    "# 4        7\n",
    "mask = np.array([[1, 1, 1, 1, 0, 0, 0],\n",
    "                 [0, 1, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "coherence_test(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests of the loss function\n",
    "Compare the result with the official PyTorch implementation. Indeed the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836884"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "                 \n",
    "# y_probs = np.array([[0.35, 0.03, 0.6, 0.08, 0.1],\n",
    "#                     [0.9, 0.03, 0.05, 0.98, 0.01],\n",
    "#                     [0.57, 0.33, 0.7, 0.2, 0.12]], dtype=np.float32)\n",
    "\n",
    "y_probs = np.array([[0.35, 0.03, 0.6, 0.08, 0.1]], dtype=np.float32)\n",
    "\n",
    "labels = np.array([[0, 0, 1, 0, 1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "term_1 = (1 - labels) * max_constrain(y_probs, mask)\n",
    "term_2 = labels * max_constrain(labels * y_probs, mask)\n",
    "y_probs_constrained = term_1 + term_2\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce(labels, y_probs_constrained).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7f323a1c2c8b3d38dc94a01188981c510c9b5df10e2cc2d7fa4f2b45d318cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
