{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import help_functions as hf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "tf.config.threading.set_intra_op_parallelism_threads(10) \n",
    "tf.config.threading.set_inter_op_parallelism_threads(10) \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_output = (1 - labels) * get_constr_out(output, M) + labels * get_constr_out(labels * output, M)\n",
    "\n",
    "# TODO: add an MCM (maximum constraint module) layer here. Hierarchy constraint expressed in matrix R\n",
    "# it seems the MCM layer only is used at inference? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 non-validated image filenames belonging to 40 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matvieir/wiki_image_classification/src/classification/help_functions.py:218: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_x_labels['labels'] = df['labels'].apply(lambda labels_list: [label for label in labels_list if label in top_classes])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37088 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "with open('training_configurations.json', 'r') as fp:\n",
    "    config = json.load(fp)[str(1)]\n",
    "\n",
    "config['batch_size'] = 1\n",
    "config['nr_classes'] = 3\n",
    "test = hf.get_flow(df_file=config['data_folder'] + '/test_df.json.bz2',\n",
    "                   nr_classes=config['nr_classes'],\n",
    "                   batch_size=config['batch_size'],\n",
    "                   image_dimension=config['image_dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) # forward pass\n",
    "            # Compute loss\n",
    "            bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "            loss = bce(y, y_pred)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "def get_uncompiled_model():\n",
    "    basemodel = EfficientNetB2(include_top=False, weights=None, classes=20, input_shape=(32, 32, 3))\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = basemodel(inputs)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(20, activation='sigmoid')(x)\n",
    "    model = CustomModel(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# model = get_compiled_model()\n",
    "# model.fit(test, verbose=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel = EfficientNetB2(include_top=False, weights=None, classes=config['nr_classes'], input_shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "inputs = Input(shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "x = basemodel(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(config['nr_classes'], activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=['categorical_accuracy'], loss='binary_crossentropy')\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "metrics = tf.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_ij = 1 if the class i is a subclass of the class j. I.e.: row i, column j is 1.\n",
    "# - note that if M_ij is 1 then M_ji is necessarily 0 as the subclass relation is not reflexive\n",
    "mask = np.array([[1, 0, 0],\n",
    "                 [1, 1, 0],\n",
    "                 [1, 0, 1]], dtype=np.float32)\n",
    "np.fill_diagonal(mask, 1)\n",
    "mask = np.transpose(mask) # TODO correct? Looking at results when only having 2 or 3 classes, yes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_constrain(output, mask):\n",
    "    \"Constrains the output given the hierarchy expressed by the mask.\"\n",
    "    batch_size = len(output) # or output.shape[0]\n",
    "    nr_classes = mask.shape[0] # = mask.shape[1] = output.shape[1]\n",
    "\n",
    "    output = tf.expand_dims(output, axis=1) # Pytorch unsqueeze()\n",
    "\n",
    "    batch_output = tf.broadcast_to(output, [batch_size, nr_classes, nr_classes]) # this is H in the MCM equation\n",
    "    batch_mask   = tf.broadcast_to(mask,   [batch_size, nr_classes, nr_classes])\n",
    "\n",
    "    constrained_output = tf.math.reduce_max(batch_output * batch_mask, axis=2)\n",
    "\n",
    "    return constrained_output\n",
    "\n",
    "@tf.function   # to speed up: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_probs = model(inputs, training=True)\n",
    "\n",
    "        # Extra steps for coherent HMC:\n",
    "        # 1. max constraint module        \n",
    "        term_1 = (1 - labels) * max_constrain(y_probs, mask)\n",
    "        term_2 = labels * max_constrain(labels * y_probs, mask)\n",
    "        y_probs_constrained = term_1 + term_2\n",
    "\n",
    "        # 2. modify what is sent to binary cross-entropy function (not anymore y_true and y_probs)\n",
    "        loss_value = model.compiled_loss(labels, y_probs_constrained)\n",
    "        # loss_value = loss_fn(labels, y_probs_constrained)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    # optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    metrics.update_state(labels, y_probs_constrained)\n",
    "    return loss_value\n",
    "    \n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}\\n')\n",
    "    end = time.time()\n",
    "    # Iterate over the batches of the dataset\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        \n",
    "        if step % 300 == 0:\n",
    "            print(f\"Training loss (for one batch) at step {step}: {float(loss_value):.4f}\")\n",
    "            print(f\"Seen so far: {(step + 1) * config['batch_size']} samples\")\n",
    "            temp_end = time.time()\n",
    "            print(f'time: {temp_end-end}')\n",
    "            end = time.time()\n",
    "\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = metrics.result()\n",
    "    print(f\"Training acc over epoch: {float(train_acc)}\")\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    metrics.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit test of the Maximum Constraint Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test of maximum constraint module\n",
    "\n",
    "# Class 1 is the parent of classes 2 and 3\n",
    "mask = np.array([[1, 0, 0],\n",
    "                 [1, 1, 0],\n",
    "                 [1, 0, 1]], dtype=np.float32)\n",
    "np.fill_diagonal(mask, 1)\n",
    "mask = np.transpose(mask)\n",
    "\n",
    "# Case 1: the parent has greater probability than both children. Nothing changes.\n",
    "probs = np.array([[0.7, 0.5, 0.2]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == probs).all())\n",
    "\n",
    "# Case 2: the parent has smaller probability than child class 2. \n",
    "probs = np.array([[0.1, 0.5, 0.2]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.5, 0.5, 0.2], dtype=np.float32)).all())\n",
    "\n",
    "# Case 3: the parent has smaller probability than child class 3. \n",
    "probs = np.array([[0.1, 0.3, 0.8]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.8, 0.3, 0.8], dtype=np.float32)).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7f323a1c2c8b3d38dc94a01188981c510c9b5df10e2cc2d7fa4f2b45d318cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
