{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'setup_gpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-688031a5e549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_nr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'setup_gpu'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import help_functions as hf\n",
    "import numpy as np\n",
    "import time\n",
    "hf.setup_gpu(gpu_nr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_output = (1 - labels) * get_constr_out(output, M) + labels * get_constr_out(labels * output, M)\n",
    "\n",
    "# TODO: add an MCM (maximum constraint module) layer here. Hierarchy constraint expressed in matrix R\n",
    "# it seems the MCM layer only is used at inference? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 non-validated image filenames belonging to 40 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matvieir/wiki_image_classification/src/classification/help_functions.py:218: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_x_labels['labels'] = df['labels'].apply(lambda labels_list: [label for label in labels_list if label in top_classes])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37088 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "with open('training_configurations.json', 'r') as fp:\n",
    "    config = json.load(fp)[str(1)]\n",
    "\n",
    "config['batch_size'] = 1\n",
    "config['nr_classes'] = 3\n",
    "test = hf.get_flow(df_file=config['data_folder'] + '/test_df.json.bz2',\n",
    "                   nr_classes=config['nr_classes'],\n",
    "                   batch_size=config['batch_size'],\n",
    "                   image_dimension=config['image_dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "\n",
    "        print(f'len(data): {len(data)}')\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_probs = model(inputs, training=True)\n",
    "\n",
    "            # Extra steps for coherent HMC:\n",
    "            # 1. max constraint module        \n",
    "            term_1 = (1 - labels) * max_constrain(y_probs, mask)\n",
    "            term_2 = labels * max_constrain(labels * y_probs, mask)\n",
    "            y_probs_constrained = term_1 + term_2\n",
    "\n",
    "            # 2. modify what is sent to binary cross-entropy function (not anymore y_true and y_probs)\n",
    "            loss_value = model.compiled_loss(labels, y_probs_constrained)\n",
    "            # loss_value = loss_fn(labels, y_probs_constrained)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        model.metrics.update_state(labels, y_probs_constrained)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "def get_uncompiled_model():\n",
    "    basemodel = EfficientNetB2(include_top=False, weights=None, classes=20, input_shape=(32, 32, 3))\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = basemodel(inputs)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(20, activation='sigmoid')(x)\n",
    "    model = CustomModel(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'], loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(test, verbose=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel = EfficientNetB2(include_top=False, weights=None, classes=config['nr_classes'], input_shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "inputs = Input(shape=(config['image_dimension'], config['image_dimension'], 3))\n",
    "x = basemodel(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(config['nr_classes'], activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=['categorical_accuracy'], loss='binary_crossentropy')\n",
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# metrics = tf.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_ij = 1 if the class j is a subclass of the class i. I.e.: row i, column j is 1.\n",
    "# - note that if M_ij is 1 then M_ji is necessarily 0 as the subclass relation is not reflexive\n",
    "# - note that M_ij is also 1 if j is not a direct descendant of i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_constrain(output, mask):\n",
    "    \"Constrains the output given the hierarchy expressed by the mask.\"\n",
    "\n",
    "    batch_size = len(output) # or output.shape[0]\n",
    "    nr_classes = mask.shape[0] # = mask.shape[1] = output.shape[1]\n",
    "\n",
    "    output = tf.expand_dims(output, axis=1) # Pytorch unsqueeze()\n",
    "\n",
    "    batch_output = tf.broadcast_to(output, [batch_size, nr_classes, nr_classes]) # this is H in the MCM equation\n",
    "    batch_mask   = tf.broadcast_to(mask,   [batch_size, nr_classes, nr_classes])\n",
    "\n",
    "    constrained_output = tf.math.reduce_max(batch_output * batch_mask, axis=2)\n",
    "\n",
    "    return constrained_output\n",
    "\n",
    "# @tf.function   # to speed up: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction\n",
    "def train_step(inputs, labels):\n",
    "\n",
    "    print(f'INPUTS: \\n{inputs}')\n",
    "    print(f'LABELS: \\n{labels}')\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_probs = model(inputs, training=True)\n",
    "\n",
    "        # Extra steps for coherent HMC:\n",
    "        # 1. max constraint module        \n",
    "        term_1 = (1 - labels) * max_constrain(y_probs, mask)\n",
    "        term_2 = labels * max_constrain(labels * y_probs, mask)\n",
    "        y_probs_constrained = term_1 + term_2\n",
    "\n",
    "        # 2. modify what is sent to binary cross-entropy function\n",
    "        loss_value = model.compiled_loss(labels, y_probs_constrained)\n",
    "        # loss_value = loss_fn(labels, y_probs_constrained)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    # optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    model.metrics.update_state(labels, y_probs_constrained)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f'Start of epoch {epoch}\\n')\n",
    "    end = time.time()\n",
    "    # Iterate over the batches of the dataset\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "        print(f'LOSS_VALUE: \\n{loss_value}')\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 300 == 0:\n",
    "            print(f\"Training loss (for one batch) at step {step}: {float(loss_value):.4f}\")\n",
    "            print(f\"Seen so far: {(step + 1) * config['batch_size']} samples\")\n",
    "            temp_end = time.time()\n",
    "            print(f'time: {temp_end-end}')\n",
    "            end = time.time()\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = model.metrics.result()\n",
    "    print(f\"Training acc over epoch: {float(train_acc)}\")\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    model.metrics.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests of the Maximum Constraint Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test of maximum constraint module: 3 classes\n",
    "# Class 1 is the parent of classes 2 and 3\n",
    "mask = np.array([[1, 0, 0],\n",
    "                 [1, 1, 0],\n",
    "                 [1, 0, 1]], dtype=np.float32)\n",
    "np.fill_diagonal(mask, 1)\n",
    "mask = np.transpose(mask)\n",
    "\n",
    "# Case 1: the parent has greater probability than both children. Nothing changes.\n",
    "probs = np.array([[0.7, 0.5, 0.2]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == probs).all())\n",
    "\n",
    "# Case 2: the parent has smaller probability than child class 2. \n",
    "probs = np.array([[0.1, 0.5, 0.2]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.5, 0.5, 0.2], dtype=np.float32)).all())\n",
    "\n",
    "# Case 3: the parent has smaller probability than child class 3. \n",
    "probs = np.array([[0.1, 0.3, 0.8]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.8, 0.3, 0.8], dtype=np.float32)).all())\n",
    "\n",
    "# Case 4: the parent has smaller probability than both children. \n",
    "probs = np.array([[0.2, 0.3, 0.5]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.5, 0.3, 0.5], dtype=np.float32)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.7 0.5 0.2 0.1 0.1]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Unit test of maximum constraint module: 5 classes\n",
    "#     1  2\n",
    "#    / \\  \\\n",
    "#   3   4  5\n",
    "# Class 1 is the parent of classes 3 and 4; class 2 is the parent of class 5\n",
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "# Case 1: all is good (1 has great prob than 3 and 4; 2 has greater prob than 5)\n",
    "probs = np.array([[0.7, 0.5, 0.2, 0.1, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == probs).all())\n",
    "\n",
    "# Case 2: 1 has smaller prob than 3; 2 is ok\n",
    "probs = np.array([[0.7, 0.5, 0.9, 0.1, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.5, 0.9, 0.1, 0.1], dtype=np.float32)).all())\n",
    "\n",
    "# Case 3: 1 has smaller prob than 4; 2 is ok\n",
    "probs = np.array([[0.7, 0.5, 0.6, 0.9, 0.1]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.5, 0.6, 0.9, 0.1], dtype=np.float32)).all())\n",
    "\n",
    "# Case 4: 1 has smaller prob than 4; 2 has smaller prob than 5\n",
    "probs = np.array([[0.7, 0.5, 0.6, 0.9, 0.8]], dtype=np.float32)\n",
    "assert((max_constrain(probs, mask)[0].numpy() == np.array([0.9, 0.8, 0.6, 0.9, 0.8], dtype=np.float32)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_child_dict(mask):\n",
    "    dict = {}\n",
    "    for row_idx in range(mask.shape[0]):\n",
    "        children_idx = np.nonzero(mask[row_idx, :])[0]\n",
    "        if children_idx.size:\n",
    "            dict[row_idx] = children_idx\n",
    "    return dict\n",
    "\n",
    "def is_coherent(probs, parent_idx, children_idx):\n",
    "    for child_idx in children_idx:\n",
    "        assert(probs[parent_idx] >= probs[child_idx])\n",
    "\n",
    "def coherence_test(mask, nr_tests=100):\n",
    "    parent_child_dict = get_parent_child_dict(mask)\n",
    "    for _ in range(nr_tests):\n",
    "        probs = np.random.uniform(low=0.0, high=1.0, size=(1, mask.shape[0]))\n",
    "        probs = np.array(probs, dtype=np.float32)\n",
    "        coherent_probs = max_constrain(probs, mask)[0] # '0' because the output of max_constrain is a copy of the same coherent probs in several lines\n",
    "        for parent_idx in parent_child_dict:\n",
    "            is_coherent(coherent_probs, parent_idx, parent_child_dict[parent_idx])  \n",
    "    print('Test passed, all probs are coherent!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   1   2\n",
    "#  / \\   \\\n",
    "# 3   4   5\n",
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask)    \n",
    "\n",
    "#     1\n",
    "#  / / \\ \\\n",
    "# 2 3  4  5\n",
    "mask = np.array([[1, 1, 1, 1, 1],\n",
    "                 [0, 1, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask)   \n",
    "\n",
    "#  1        5     8 \n",
    "# / \\ \\    / \\   / \\\n",
    "# 2 3  4  6   7  9  10\n",
    "mask = np.array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "\n",
    "coherence_test(mask) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees with 3 or more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# |\n",
    "# 2\n",
    "# |\n",
    "# 3\n",
    "mask = np.array([[1, 1, 1],\n",
    "                 [0, 1, 1],\n",
    "                 [0, 0, 1]], dtype=np.float32)\n",
    "coherence_test(mask)\n",
    "\n",
    "#     1    5\n",
    "#    / \\   |\n",
    "#   2   3  6\n",
    "#  /       |\n",
    "# 4        7\n",
    "mask = np.array([[1, 1, 1, 1, 0, 0, 0],\n",
    "                 [0, 1, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 1, 1],\n",
    "                 [0, 0, 0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "coherence_test(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests of the loss function\n",
    "Compare the result with the official PyTorch implementation. Indeed the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836884"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array([[1, 0, 1, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 1],], dtype=np.float32)\n",
    "                 \n",
    "# y_probs = np.array([[0.35, 0.03, 0.6, 0.08, 0.1],\n",
    "#                     [0.9, 0.03, 0.05, 0.98, 0.01],\n",
    "#                     [0.57, 0.33, 0.7, 0.2, 0.12]], dtype=np.float32)\n",
    "\n",
    "y_probs = np.array([[0.35, 0.03, 0.6, 0.08, 0.1]], dtype=np.float32)\n",
    "\n",
    "labels = np.array([[0, 0, 1, 0, 1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "term_1 = (1 - labels) * max_constrain(y_probs, mask)\n",
    "term_2 = labels * max_constrain(labels * y_probs, mask)\n",
    "y_probs_constrained = term_1 + term_2\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce(labels, y_probs_constrained).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7f323a1c2c8b3d38dc94a01188981c510c9b5df10e2cc2d7fa4f2b45d318cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
