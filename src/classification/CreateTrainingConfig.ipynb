{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of classes: 10, 20, 30, 40\n",
    "- Basemodel: EfficientNetB0, EfficientNetB1, Xception\n",
    "- Class weights: true/false\n",
    "- Number of training data\n",
    "- Naive labels vs ORES labels\n",
    "- Affine augmentations of training set\n",
    "<!-- - Dimension to which image is reduced (now 32x32) -->\n",
    "<!-- - Unfrozen layers -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file...\n"
     ]
    }
   ],
   "source": [
    "print('Creating file...')\n",
    "training_configurations = dict()\n",
    "    \n",
    "def create_training_entry(i, class_weights=False, augment=False, undersample=False, oversample=False, random_initialization=False,\n",
    "                          data_folder='data/split_dataframes_heuristic_labels_20221006', \n",
    "                          epochs=15, basemodel='EfficientNetB2', nr_classes=20, batch_size=512, \n",
    "                          loss_function='binary_crossentropy', image_dimension=64,\n",
    "                          number_trainable_layers=0):\n",
    "    print(f'Creating {i}th entry in the training_configurations dictionary.')\n",
    "    training_configurations[i] = dict()\n",
    "    training_configurations[i]['epochs'] = epochs\n",
    "    training_configurations[i]['augment'] = augment\n",
    "    training_configurations[i]['basemodel'] = basemodel\n",
    "    training_configurations[i]['nr_classes'] = nr_classes\n",
    "    training_configurations[i]['batch_size'] = batch_size\n",
    "    training_configurations[i]['undersample'] = undersample\n",
    "    training_configurations[i]['oversample'] = oversample\n",
    "    training_configurations[i]['data_folder'] = data_folder\n",
    "    training_configurations[i]['loss_function'] = loss_function\n",
    "    training_configurations[i]['class_weights'] = class_weights\n",
    "    training_configurations[i]['image_dimension'] = image_dimension\n",
    "    training_configurations[i]['random_initialization'] = random_initialization\n",
    "    training_configurations[i]['number_trainable_layers'] = number_trainable_layers\n",
    "    results_folder = f'results_thesis/{i}'\n",
    "    results_folder += '_augment_' if augment == True else ''\n",
    "    results_folder += '_undersample_' if undersample == True else ''\n",
    "    results_folder += '_oversample_' if oversample == True else ''\n",
    "    results_folder += '_class_weights_' if class_weights == True else ''\n",
    "    results_folder += '_random_initialization_' if random_initialization == True else ''\n",
    "    results_folder += f'_bs_{batch_size}'\n",
    "    training_configurations[i]['results_folder'] = results_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 0th entry in the training_configurations dictionary.\n",
      "Creating 1th entry in the training_configurations dictionary.\n",
      "Creating 10th entry in the training_configurations dictionary.\n",
      "Creating 11th entry in the training_configurations dictionary.\n",
      "Creating 20th entry in the training_configurations dictionary.\n"
     ]
    }
   ],
   "source": [
    "create_training_entry(i=0, random_initialization=False, number_trainable_layers=339)\n",
    "create_training_entry(i=1, random_initialization=True, number_trainable_layers=339)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_training_entry(i=10, \n",
    "                      random_initialization=False, \n",
    "                      number_trainable_layers=0, \n",
    "                      batch_size=32, \n",
    "                      undersample=True)\n",
    "\n",
    "create_training_entry(i=11, \n",
    "                      random_initialization=False, \n",
    "                      number_trainable_layers=0, \n",
    "                      batch_size=32, \n",
    "                      oversample=True)\n",
    "\n",
    "create_training_entry(i=20, \n",
    "                      data_folder='data/split_hierarchical_data_221115',\n",
    "                      nr_classes='all')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('training_configurations.json', 'w') as fp:\n",
    "    json.dump(training_configurations, fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7f323a1c2c8b3d38dc94a01188981c510c9b5df10e2cc2d7fa4f2b45d318cbd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
