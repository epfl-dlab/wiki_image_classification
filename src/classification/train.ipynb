{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import help_functions as hf\n",
    "from configs import configs\n",
    "import urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n",
      "{'batch_size': 64, 'epochs': 100, 'image_dimension': 64, 'monitor': 'pr_auc', 'random_initialization': False, 'class_weights': False, 'hierarchical': False, 'loss_function': 'sample_weight', 'data_folder': '/home/matvieir/wiki_image_classification/src/classification/data/jpg-data', 'results_folder': 'results_paper/230703_sample_weight_100epochs'}\n",
      "Found 70000 validated image filenames belonging to 28 classes.\n",
      "LOG: finished getting training flow\n",
      "Found 7500 validated image filenames belonging to 28 classes.\n",
      "val_stop_df shape: (7500, 5)\n",
      "LOG: Got the validation flow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hf.setup_gpu(gpu_nr=0) # if some of the GPUs is busy, choose one (0 or 1)\n",
    "\n",
    "# ===========================================\n",
    "# =========== HYPER-PARAMETERS ==============\n",
    "# ===========================================\n",
    "\n",
    "config = configs[0]\n",
    "print(config)\n",
    "os.mkdir(config['results_folder'])\n",
    "\n",
    "# Save outputs to log file\n",
    "# old_stdout = sys.stdout\n",
    "# log_file = open(config['results_folder'] + '/log.txt', 'w')\n",
    "# sys.stdout = log_file\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ================= LOAD DATA ================\n",
    "# ============================================\n",
    "train, _ = hf.get_flow(df_file=config['data_folder'] + '/train_df.json.bz2',\n",
    "                              batch_size=config['batch_size'],\n",
    "                              image_dimension=config['image_dimension'])\n",
    "print('LOG: finished getting training flow')\n",
    "\n",
    "y_true = hf.get_y_true(shape=(train.samples, len(train.class_indices)), classes=train.classes)\n",
    "\n",
    "val_stop, val_stop_df = hf.get_flow(df_file=config['data_folder'] + '/val_df.json.bz2',\n",
    "                          batch_size=config['batch_size'],\n",
    "                          image_dimension=config['image_dimension'])\n",
    "print(f'val_stop_df shape: {val_stop_df.shape}')\n",
    "print('LOG: Got the validation flow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- \n",
      "human_df shape: (327, 5)\n",
      "\n",
      "\n",
      "\n",
      "Found 285 validated image filenames belonging to 28 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matvieir/anaconda3/lib/python3.8/site-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 42 invalid image filename(s) in x_col=\"url\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load human-labeled set\n",
    "def get_human_flow(human_df_address):\n",
    "    human_df = pd.read_parquet(human_df_address)\n",
    "    human_df['labels'] = human_df.apply(lambda x: list(x.labels), axis=1) # otherwise the labels column will be a list of lists\n",
    "    human_df['url'] = human_df.apply(lambda x: '/scratch/WIT_Dataset/images/' + x.url.split('/wikipedia/commons/')[1], axis=1)\n",
    "    human_df['url'] = human_df['url'].apply(lambda encoded_filename : urllib.parse.unquote(encoded_filename).encode().decode('unicode-escape'))\n",
    "    print(f'----------------------- \\nhuman_df shape: {human_df.shape}\\n\\n\\n')\n",
    "    human, human_df = hf.get_flow(df=human_df, batch_size=config['batch_size'], image_dimension=config['image_dimension'])\n",
    "    return human, human_df\n",
    "human, human_df = get_human_flow('../../data/evaluation/annotated_validation.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: creating model\n",
      "\n",
      "Number of layers in basemodel: 339\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb2 (Functional)  (None, 2, 2, 1408)        7768569   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5632)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               721024    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                3612      \n",
      "=================================================================\n",
      "Total params: 8,493,205\n",
      "Trainable params: 8,425,630\n",
      "Non-trainable params: 67,575\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ============= CREATE MODEL =================\n",
    "# ============================================\n",
    "print('LOG: creating model')\n",
    "start = time.time()\n",
    "model = hf.create_model(n_labels=len(train.class_indices), \n",
    "                        image_dimension=config['image_dimension'],\n",
    "                        random_initialization=config['random_initialization'],\n",
    "                        loss=config['loss_function'],\n",
    "                        y_true=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.reset()\n",
    "# val_stop.reset()\n",
    "# human.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# class EvaluateCallback(tf.keras.callbacks.Callback):\n",
    "#     def __init__(self, human_dataflow, val_dataflow, history_csv_path):\n",
    "#         super(EvaluateCallback, self).__init__()\n",
    "#         # self.human_dataflow = human_dataflow\n",
    "#         # self.val_dataflow = val_dataflow\n",
    "#         self.history_csv_path = history_csv_path\n",
    "#         print('model summary from EvaluateCallback')\n",
    "#         model.summary()\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "#         print('\\n---------- PREDICTING VAL ---------')\n",
    "#         # val_evaluation_results = model.evaluate(val_stop, verbose=1)\n",
    "#         # print(f'\\nval_evaluation_results: {val_evaluation_results}')\n",
    "\n",
    "\n",
    "#         print('\\n---------- PREDICTING HUMAN ---------')\n",
    "#         human_evaluation_results = model.evaluate(human, verbose=1)\n",
    "#         print(f'\\nhuman_evaluation_results: {human_evaluation_results}')\n",
    "#         # human_loss = human_evaluation_results[0]\n",
    "#         # human_metrics_values = human_evaluation_results[1:]\n",
    "\n",
    "#         # with open(self.history_csv_path, 'a') as file:\n",
    "#         #     writer = csv.writer(file)\n",
    "#         #     writer.writerow([epoch] + [human_loss] + human_metrics_values)\n",
    "\n",
    "import csv\n",
    "class EvaluateCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, human_dataflow, history_csv_path):\n",
    "        super(EvaluateCallback, self).__init__()\n",
    "        self.human_dataflow = human_dataflow\n",
    "        self.history_csv_path = history_csv_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        print('\\n---------- PREDICTING HUMAN ---------')\n",
    "        human_evaluation_results = self.model.evaluate(self.human_dataflow, verbose=1)\n",
    "        print(f'\\nhuman_evaluation_results: {human_evaluation_results}')\n",
    "        human_loss = human_evaluation_results[0]\n",
    "        human_metrics_values = human_evaluation_results[1:]\n",
    "\n",
    "        with open(self.history_csv_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch] + [human_loss] + human_metrics_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094\n",
      "118\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(train.__len__())\n",
    "print(val_stop.__len__())\n",
    "print(human.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 1.4245 - binary_accuracy: 0.5965 - precision: 0.1367 - recall: 0.5430 - pr_auc: 0.1262\n",
      "---------- PREDICTING HUMAN ---------\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 6.1358 - binary_accuracy: 0.6192 - precision: 0.1316 - recall: 0.6046 - pr_auc: 0.1448\n",
      "\n",
      "human_evaluation_results: [6.135798931121826, 0.619172990322113, 0.13159547746181488, 0.6046175956726074, 0.1448349952697754]\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.4245 - binary_accuracy: 0.5965 - precision: 0.1367 - recall: 0.5430 - pr_auc: 0.1262\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ================ TRAIN MODEL ===============\n",
    "# ============================================\n",
    "\n",
    "history_callback = tf.keras.callbacks.CSVLogger(f\"{config['results_folder']}/history.csv\", separator=',', append=True)\n",
    "evaluate_callback = EvaluateCallback(human_dataflow=human, history_csv_path=f\"{config['results_folder']}/history_human.csv\")\n",
    "\n",
    "history = model.fit(\n",
    "        train,\n",
    "        verbose=1, # one line per epoch \n",
    "        # validation_data=val_stop,\n",
    "        steps_per_epoch=50,\n",
    "        epochs=1,\n",
    "        callbacks=[history_callback, \n",
    "                   evaluate_callback\n",
    "                   ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
